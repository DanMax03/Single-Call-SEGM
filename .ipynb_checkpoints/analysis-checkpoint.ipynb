{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25ba226",
   "metadata": {},
   "source": [
    "# Анализ статьи [On the convergence of Single-Call Stochastic Extra-Gradient Methods](https://arxiv.org/abs/1908.08465)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c3e367",
   "metadata": {},
   "source": [
    "**Latex preset**\n",
    "$\\require{amsmath}$\n",
    "\n",
    "$\\newcommand{\\eps}{\\varepsilon}$\n",
    "$\\newcommand{\\bfone}{\\mathbf{1}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\N}{\\mathbb{N}}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\bS}{\\mathbb{S}}$\n",
    "$\\newcommand{\\cC}{\\mathcal{C}}$\n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$\n",
    "$\\newcommand{\\cF}{\\mathcal{F}}$\n",
    "$\\newcommand{\\cL}{\\mathcal{L}}$\n",
    "$\\newcommand{\\cX}{\\mathcal{X}}$\n",
    "$\\newcommand{\\wdh}{\\widehat}$\n",
    "$\\newcommand{\\wdt}{\\widetilde}$\n",
    "$\\newcommand{\\vdelta}{\\partial}$\n",
    "$\\newcommand{\\pd}[2]{\\frac{\\vdelta #1}{\\vdelta #2}}$\n",
    "$\\newcommand{\\lsi}[1]{\\left[#1\\right)}$\n",
    "$\\newcommand{\\rsi}[1]{\\left(#1\\right]}$\n",
    "$\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\range}[2]{\\set{#1, \\ldots, #2}}$\n",
    "$\\newcommand{\\such}{\\ \\big|\\ }$\n",
    "$\\let\\bs\\backslash$\n",
    "$\\let\\ra\\rightarrow$\n",
    "$\\let\\Ra\\Rightarrow$\n",
    "$\\let\\Lora\\Longrightarrow$\n",
    "$\\let\\Lra\\Leftrightarrow$\n",
    "$\\let\\la\\leftarrow$\n",
    "$\\let\\La\\Leftarrow$\n",
    "$\\let\\Lola\\Longleftarrow$\n",
    "$\\let\\Lolra\\Longleftrightarrow$\n",
    "$\\let\\ole\\overline$\n",
    "$\\newcommand{\\floor}[1]{\\left\\lfloor#1\\right\\rfloor}$\n",
    "$\\newcommand{\\ceil}[1]{\\left\\lceil#1\\right\\rceil}$\n",
    "$\\newcommand{\\ps}[1]{\\left(#1\\right)}$\n",
    "$\\newcommand{\\md}[1]{\\left|#1\\right|}$\n",
    "$\\newcommand{\\nm}[1]{\\left\\|#1\\right\\|}$\n",
    "$\\newcommand{\\tbr}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "$\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\System}[1]{\\left\\{\\begin{aligned}#1\\end{aligned}\\right.}$\n",
    "$\\DeclareMathOperator{\\tr}{tr}$\n",
    "$\\DeclareMathOperator{\\cl}{cl}$\n",
    "$\\DeclareMathOperator{\\rk}{rk}$\n",
    "$\\DeclareMathOperator{\\Int}{int}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\Err}{Err}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91a6e3",
   "metadata": {},
   "source": [
    "Далее слово 'автор' относится исключительно к автору какой-либо статьи, а человек, который написал данный ноутбук, именутся как? И зачем?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b55347",
   "metadata": {},
   "source": [
    "## Причины статьи\n",
    "\n",
    "Машинное обучение (в частности GANы) нуждается в методах для решения вариациационных неравенств, которые бы требовали как можно меньше вызовов оракула (вычисление градиента, поиск проекции). Базовые методы требуют 2 вызова и градиента, и проекции за 1 итерацию.\n",
    "\n",
    "Очень часто в таких задачах либо нет ограничений на решения, либо они крайне незначительны и их можно простыми способами обойти, поэтому проекция не страшна. Вся борьба ведётся за вычисление градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83fc67",
   "metadata": {},
   "source": [
    "## Основные определения\n",
    "\n",
    "### Стохастический анализ\n",
    "\n",
    "Пусть $(\\Omega, \\cF, P)$ - вероятностной пространство, $(E, \\cE)$ - измеримое пространство. Тогда множество случайных величин $\\{X_t\\}_{t = 1}^\\infty$, $X_t \\colon \\Omega \\to E$ называется *стохастическим (случайным) процессом*.\n",
    "\n",
    "В силу исторических причин, $t$ соотносится обычно со временем.\n",
    "\n",
    "**Определение:** пусть $T \\colon \\cX \\to \\cX$ - отображение, сохраняющее вероятностную меру, $f \\in L_1(P)$ (то есть $f$ интегрируема по Лебегу по мере $P$), $x \\in \\cX$ Тогда *эргодическим средним* или же *средним по времени* называется следующая точка, если она существует:\n",
    "$$\n",
    "    \\hat{f}(x) = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k = 0}^{n - 1} f(T^k(x))\n",
    "$$\n",
    "где $T^k$ соответствует композиции $k$ раз.\n",
    "\n",
    "**В контексте алгоритмов:** $x = x_0$ - стартовая точка метода оптимизации. За $T$ можно считать 1 итерацию алгоритма, а $f$ нас обычно интересует лишь вида $f = id$.\n",
    "\n",
    "**Замечание:** мы также будем называть эргодическим средним выражение, стоящее под знаком предела.\n",
    "\n",
    "### Оптимизация\n",
    "\n",
    "Далее везде мы живём в пространстве $\\R^d$, если не сказано явно иного. Также мы закрепляем следующие обозначения:\n",
    "\n",
    "- $\\cX \\subseteq \\R^d$ за непустым замкнутым выпуклым подмножеством точек\n",
    "- $V \\colon \\R^d \\to \\R^d$ - некоторый оператор на нашем пространстве, чьи свойства мы будем уточнять по мере необходимости.\n",
    "\n",
    "**Определение:** *седловой точкой* дифференцируемой функции $f \\colon \\R^d \\to \\R^n$ называется критическая точка $f$, которая не является ни локальным минимумом, ни локальным максимумом $f$.\n",
    "\n",
    "**Определение:** Пусть $\\cX = \\Theta \\times \\Phi$, где $\\Theta \\subseteq \\R^{d_1}$ и $\\Phi \\subseteq \\R^{d_2}$. $\\cL(\\theta, \\phi)$ - дифференцируемая функция двух переменных с липшицевым градиентом. Тогда *min-max задачей* называется следующая задача оптимизации:\n",
    "$$\n",
    "    \\min_{\\theta \\in \\R^{d_1}} \\max_{\\phi \\in \\R^{d_2}} \\cL(\\theta, \\phi), \\text{ s.t. } \\theta \\in \\Theta \\wedge \\phi \\in \\Phi\n",
    "$$\n",
    "\n",
    "**Определение:** в min-max задаче, где $\\cL$ является выпукло-вогнутой функцией, имеет место *функция ошибки Никаидо-Исоды*:\n",
    "$$\n",
    "    NI(\\wdh{x}) := \\sup_{\\phi \\in \\Phi} \\cL(\\wdh{\\theta}, \\phi) - \\inf_{\\theta \\in \\Theta} \\cL(\\theta, \\wdh{\\phi})\n",
    "$$\n",
    "И также есть ограниченный вариант:\n",
    "$$\n",
    "    NI_R(\\wdh{x}) := \\max_{\\phi \\in \\Phi_R} \\cL(\\wdh{\\theta}, \\phi) - \\min_{\\theta \\in \\Theta_R} \\cL(\\theta, \\wdh{\\phi})\n",
    "$$\n",
    "где $\\Phi_R = \\{\\phi \\in \\Phi \\colon \\|\\phi\\| \\le R\\}$, $\\Theta_R = \\{\\theta \\in \\Theta \\colon \\|\\theta\\| \\le R\\}$\n",
    "\n",
    "**Определение:** оператор $V$ называется *монотонным*, если выполнено следующее условие:\n",
    "$$\n",
    "    \\forall x, x' \\in \\R^d\\ \\ \\tbr{V(x') - V(x), x' - x} \\ge 0\n",
    "$$\n",
    "\n",
    "**Интуиция:** вектор изменения значений $V$ сонаправлен с осью, задаваемой вектором $x' - x$. Для случая $V = \\nabla f$, где $f$ некоторая *достаточно гладкая* функция, выполнение условия эквивалентно тому, что $f$ выпукла\n",
    "\n",
    "**Определение:** оператор $V$ называется *$\\alpha$-строго монотонным*, если выполнено следующее условие:\n",
    "$$\n",
    "    \\exists \\alpha > 0 \\such \\forall x, x' \\in \\R^d\\ \\ \\tbr{V(x') - V(x), x' - x} \\ge \\alpha\\|x' - x\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2cdbe",
   "metadata": {},
   "source": [
    "## Основные цели\n",
    "- Обобщить уже имеющиеся результаты про 1-EG методы\n",
    "- Показать, что детерминированные 1-EG методы сохраняют оптимальный порядок сходимости $O(1 / t)$ в случае монотонного вариационного неравенства с липшицевостью.\n",
    "- Показать, что стохастические 1-EG методы сохраняют оптимальный порядок $O(1 / t)$ локальной сходимости (для последней итерации) в случае немонотонного вариационного неравенства\n",
    "\n",
    "<img src=\"assets/ArticleResults.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "Обведённые результаты - то, что было получено в статье.\n",
    "\n",
    "**Замечание:** зачем изучать и эргодическую точку, и точку последней итерации алгоритма? Потому что первое хорошо работает в монотонном случае, а второе, соответственно, в немонотонном случае (когда, например, неприменимо неравенство Йенсена)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d7daf",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "**Определение:** *вариационным неравенством (variational inequality, или просто VI)* называется задача поиска такого $x^* \\in \\cX$, что выполнено неравенство:\n",
    "$$\n",
    "    \\exists x^* \\in \\cX \\such \\forall x \\in \\cX\\ \\ \\tbr{V(x^*), x - x^*} \\ge 0\n",
    "$$\n",
    "\n",
    "**Интуиция к определению:** мы уже сталкивались с похожим неравенством, когда говорили о задаче минимизации выпуклой дифференцируемой функции $f$ на некотором выпуклом множестве $\\cX$. Тогда получалось, что $x^*$ - искомый минимум тогда и только тогда, когда выполнено неравенство:\n",
    "$$\n",
    "    \\forall x \\in \\cX \\tbr{\\nabla f(x^*), x - x^*} \\ge 0\n",
    "$$\n",
    "По сути это означает, что множество $\\cX$ лежит по одну сторону от гиперплоскости, которая задаётся своей точкой $x^*$ и нормалью $\\nabla f(x^*)$\n",
    "\n",
    "<img src=\"assets/FullConvexTheorem.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "**Примеры:**\n",
    "\n",
    "1. Задача минимизации. Если $f$ - гладкая функция потерь на $\\cX = \\R^d$, то при $V = \\nabla f$ точка $x^*$ является решением вариационного неравенства тогда и только тогда, когда $\\nabla f(x^*) = 0$.\n",
    "\n",
    "2. Min-max оптимизация. Пусть $\\cX = \\Theta \\times \\Phi$, $\\Theta = \\R^{d_1}$ и $\\Phi = \\R^{d_2}$ и задана гладкая функция $L(\\theta, \\phi)$. Тогда задачу min-max оптимизации можно переписать в виде вариационного неравенства, используя $V(x) = (\\nabla_\\theta \\cL(x), -\\nabla_\\phi \\cL(x))$\n",
    "\n",
    "**Определение:** *функцией ошибки* для потенциального решения $\\wdh{x} \\in \\cX$ вариационного неравенства мы назовём следующую функцию:\n",
    "$$\n",
    "    \\Err(\\wdh{x}) = \\sup_{x \\in \\cX} \\tbr{V(x), \\wdh{x} - x}\n",
    "$$\n",
    "и её *ограниченный* вариант:\n",
    "$$\n",
    "    \\Err_R(\\wdh{x}) = \\max_{x \\in \\cX_R} \\tbr{V(x), \\wdh{x} - x}\n",
    "$$\n",
    "где $\\cX_R = \\cX \\cap \\ole{B}_R(0) = \\{x \\in \\cX \\colon \\|x\\| \\le R\\}$\n",
    "\n",
    "**Интуиция:** самая простая функция ошибки. Ровно говорит нам о том, насколько мы близко/далеко с желаемым неравенством с нулём.\n",
    "\n",
    "**Определение:** будем говорить, что решение вариационного неравенства $x^* \\in \\cX$ является *регулярным*, если выполнены условия:\n",
    "1. Оператор $V$ является $C^1$-гладким в окрестности $x^*$\n",
    "2. Якобиан $J_V(x^*)$ является положительно определённым относительно всех касательных направлений к $\\cX$ в точке $x^*$ (множество таких направлений обозначил за $Z_{\\cX, x^*}$):\n",
    "$$\n",
    "    \\forall z \\in Z_{\\cX, x^*} \\bs \\{0\\}\\ \\ z^TJ_V(x^*)z = \\sum_{i = 1}^d \\sum_{j = 1}^d z_i \\pd{V_i}{x_j}(x^*)z_j > 0\n",
    "$$\n",
    "\n",
    "**Интуиция:** эти условия соотносятся с требованиями на гессиан, если мы рассматриваем задачу $V = \\nabla f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5371e16",
   "metadata": {},
   "source": [
    "**Лемма (Нестеров, 2007):** пусть $V$ монотонна. Тогда $x^* \\in \\cX$ является решением вариационного неравенства тогда и только тогда, когда $\\Err(x^*) = 0$.\n",
    "\n",
    "Более того, так как $\\Err(x^*) = 0$ влечёт за собой $\\Err_R(x^*) = 0$ для достаточно большого $R > 0$, то наличие условия $\\Err_R(x^*) = 0$ также является достаточным для того, чтобы $x^* \\in \\cX_R$ было решением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40071f",
   "metadata": {},
   "source": [
    "## Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd6324",
   "metadata": {},
   "source": [
    "**Допущение:** Далее мы столкнёмся с изучением *стохастических (случайных) алгоритмов*. Все они будут генерировать последовательности точек $X_t$ из пространства $\\cX$ (говоря явно, $X_t \\colon \\Omega \\to \\cX$, где $\\Omega$ относится к множеству исходов вероятностного пространства). Случайность возникает из-за использования *стохастического оракула*, чьё поведение мы моделируем так:\n",
    "$$\n",
    "    V_t = V(X_t) + Z_t\n",
    "$$\n",
    "где $Z_t \\in \\R^d$ - случайный шум. Если $\\cF_t$ - это натуральная фильтрация $X_t$, то мы допускаем следующие условия:\n",
    "1. $\\forall t\\ \\ \\E(Z_t | \\cF_t) = 0$\n",
    "\n",
    "2. $\\forall t\\ \\ \\E(\\|Z_t\\|^2| \\cF_t) \\le \\sigma^2$\n",
    "\n",
    "Понятно, что если $\\sigma^2 = 0$, то случайный процесс вырождается в детерминированный, всегда верно равенство $V_t = V(X_t)$. Далее мы будем использовать переменные случайных точек, но алгоритмы, описанные таким образом, естественно можно рассматривать как детерминированные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ccd8f",
   "metadata": {},
   "source": [
    "### Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c95e7",
   "metadata": {},
   "source": [
    "**Экстра-градиентный спуск (Extra-Gradient algorithm):** (t + 1)-итерация для EG-алгоритма записывается следующим образом:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        &{X_{t + 1 / 2} = \\Pi_\\cX(X_t - \\gamma_tV_t)}\n",
    "        \\\\\n",
    "        &{X_{t + 1} = \\Pi_\\cX(X_t - \\gamma_tV_{t + 1 / 2})}\n",
    "    \\end{aligned}\n",
    "$$\n",
    "где \n",
    "- $\\Pi_\\cX$ - обыкновенная евклидова проекция на $\\cX$: $\\Pi_\\cX(y) := \\arg \\min_{x \\in \\cX} \\|y - x\\|$\n",
    "- $\\gamma_t > 0$ - произвольно выбранный шаг итерации\n",
    "\n",
    "**Замечание:** отличие от обычного градиентного спуска состоит в том, что здесь мы находим *промежуточную точку (её ещё называют ведущей)* $X_{t + 1 / 2}$, а затем по информации в ней строим следующую *базовую точку* $X_t$, используя при этом один и тот же шаг $\\gamma_t$. В своей идее экстраградиентный метод крайне напоминает метод Нестерова.\n",
    "\n",
    "**Экстра-градиентный спуск с одним вызовом (оракула):** существуют разные способы убрать градиент из итерации. Так, в статье рассмотрены следующие 3 (они используют либо предыдущий, уже посчитанный градиент, либо аппроксимируют его через разность уже имеющихся $X_t$):\n",
    "1. Экстра-градиентный спуск по прошлому оракулу (Past Extra-Gradient, PEG):\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{t + 1 / 2} = \\Pi_\\cX(X_t - \\gamma_tV_{t - 1 / 2})}\n",
    "            \\\\\n",
    "            &{X_{t + 1} = \\Pi_\\cX(X_t - \\gamma_tV_{t + 1 / 2})}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    **Идея:** используем $V_{t - 1 / 2}$ вместо того, чтобы считать $V_t$ при вычислении $X_{t + 1 / 2}$\n",
    "\n",
    "2. Отражённый градиентный спуск (Reflected Gradient, RG):\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{t + 1 / 2} = X_t - (X_{t - 1} - X_t)}\n",
    "            \\\\\n",
    "            &{X_{t + 1} = \\Pi_\\cX(X_t - \\gamma_tV_{t + 1 / 2})}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    **Идея:** используем $\\frac{X_{t - 1} - X_t}{\\gamma_t}$ вместо $V_t$ и отказываемся от проекции при вычислении $X_{t + 1 / 2}$\n",
    "\n",
    "3. Оптимистичный градиентный спуск (Optimistic Gradient, OG):\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{t + 1 / 2} = \\Pi_\\cX(X_t - \\gamma_tV_{t - 1 / 2})}\n",
    "            \\\\\n",
    "            &{X_{t + 1} = X_{t + 1 / 2} + \\gamma_tV_{t - 1 / 2} - \\gamma_tV_{t + 1 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    **Идея:** используем $V_{t - 1 / 2}$ вместо $V_t$ при вычислении $X_{t + 1 / 2}$. Используем $X_{t + 1 / 2} + \\gamma_tV_{t - 1 / 2}$ вместо $X_t$ и отказываемся от проекции при вычислении $X_{t + 1}$\n",
    "    \n",
    "Далее, чтобы говорить об этих методах сразу, мы будем называть их *1-EG методами* или *1-EG алгоритмами*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba6029",
   "metadata": {},
   "source": [
    "**Утверждение:** если все 1-EG детерминированные методы имеют одинаковые стартовые значения $X_0 = X_1 \\in \\cX$, $V_{1 / 2} = 0$ и запускаются с постоянным шагом $\\gamma_t = \\gamma$, то в случае $\\cX = \\R^d$ точки $X_t$ на каждой итерации у всех 1-EG совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95c6ad",
   "metadata": {},
   "source": [
    "**Доказательство:** индукция по $t$. Отметим, что коль скоро $\\cX = \\R^d$, то оператор проекции является просто тождественным, его можно не писать:\n",
    "- База индукции $t \\le 2$:\n",
    "    - PEG:\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{3 / 2} = X_1 - \\gamma V_{1 / 2} = X_0}\n",
    "            \\\\\n",
    "            &{X_2 = X_1 - \\gamma V_{3 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    - RG:\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{3 / 2} = X_1 - (X_0 - X_1) = X_1 = X_0}\n",
    "            \\\\\n",
    "            &{X_2 = X_1 - \\gamma V_{3 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    - OG:\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{3 / 2} = X_1 - \\gamma V_{1 / 2} = X_0}\n",
    "            \\\\\n",
    "            &{X_2 = X_{3 / 2} + \\gamma V_{1 / 2} - \\gamma V_{3 / 2} = X_1 - \\gamma V_{3 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "- Переход $t > 2$: покажем, что правила приводятся к одинаковому виду, тем самым точки получатся одинаковые.\n",
    "    - PEG:\n",
    "      $$\n",
    "          \\begin{aligned}\n",
    "              &{X_{t + 1 / 2} = X_t - \\gamma V_{t - 1 / 2}}\n",
    "              \\\\\n",
    "              &{X_{t + 1} = X_t - \\gamma V_{t + 1 / 2}}\n",
    "          \\end{aligned}\n",
    "      $$\n",
    "    - RG:\n",
    "      $$\n",
    "          \\begin{aligned}\n",
    "              &{X_{t + 1 / 2} = X_t - (X_{t - 1} - X_t)}\n",
    "              \\\\\n",
    "              &{X_{t + 1} = X_t - \\gamma V_{t + 1 / 2}}\n",
    "          \\end{aligned}\n",
    "      $$\n",
    "      Преобразуем первое правило. Для этого распишем $X_t$ через второе правило:\n",
    "      $$\n",
    "          X_t = X_{t - 1} - \\gamma V_{t - 1 + 1 / 2} = X_{t - 1} - \\gamma V_{t - 1 / 2} \\Lora X_{t - 1} - X_t = \\gamma V_{t - 1 / 2}\n",
    "      $$\n",
    "      Подстановка тривиально делает первое правило RG таким же, как и у PEG'а\n",
    "    - OG:\n",
    "      $$\n",
    "          \\begin{aligned}\n",
    "              &{X_{t + 1 / 2} = X_t - \\gamma V_{t - 1 / 2} \\Lora X_{t + 1 / 2} + \\gamma V_{t - 1 / 2} = X_t}\n",
    "              \\\\\n",
    "              &{X_{t + 1} = X_{t + 1 / 2} + \\gamma V_{t - 1 / 2} - \\gamma V_{t + 1 / 2} \\Lora X_{t + 1} = X_t - \\gamma V_{t + 1 / 2}}\n",
    "          \\end{aligned}\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b13fd8",
   "metadata": {},
   "source": [
    "### Технические леммы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6829346",
   "metadata": {},
   "source": [
    "**Замечание:** дальше будет довольно удобно использовать нотацию для подстановки переменных. Так, выражение $(x_1, \\ldots, x_n) \\la (y_1, \\ldots, y_n)$ означает подстановку вместо переменных $x_1, \\ldots, x_n$ переменные $y_1, \\ldots, y_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df96598",
   "metadata": {},
   "source": [
    "**Техлемма 0:** имеет место так называемое *неравенство Юнга*:\n",
    "$$\n",
    "    \\forall x, y \\in \\R^d\\ \\ 2\\tbr{x, y} \\le \\|x\\|^2 + \\|y\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40d587",
   "metadata": {},
   "source": [
    "**Доказательство:** заметим, что если перенести всё направо, то получим $\\|x - y\\|^2 \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc874a",
   "metadata": {},
   "source": [
    "**Следствие:** верно неравенство $\\forall x, y \\in \\R^d\\ \\ \\|x + y\\|^2 \\le 2\\|x\\|^2 + 2\\|y\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecb89b",
   "metadata": {},
   "source": [
    "**Техлемма 1:** пусть $x, y \\in \\R^d$ и $\\cC \\subseteq \\R^d$ - замкнутое выпуклое множество. Определим $x^+ := \\Pi_\\cC(x - y)$. Тогда верно следующее неравенство:\n",
    "$$\n",
    "    \\forall p \\in \\cC\\ \\ \\|x^+ - p\\|^2 \\le \\|x - p\\|^2 - 2\\tbr{y, x^+ - p} - \\|x^+ - x\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0ec3b",
   "metadata": {},
   "source": [
    "**Доказательство:** распишем величину слева в неравенстве:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        &\\|x^+ - p\\|^2 = \\|x^+ - x + x - p\\|^2 =\n",
    "        \\\\\n",
    "        &\\|x - p\\|^2 + 2\\tbr{x^+ - x, x - p} + \\|x^+ - x\\|^2 =\n",
    "        \\\\\n",
    "        &\\|x - p\\|^2 + 2\\tbr{x^+ - x, x^+ - p} + \\|x^+ - x\\|^2\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Проекция, как известно, обладает хорошим свойством на скалярное произведение: $\\tbr{x^+ - (x - y), x^+ - p} \\le 0$. Из этого неравенства выражаем то скалярное произведение, что наисано выше и тривиально получаем требуемое."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5744074",
   "metadata": {},
   "source": [
    "**Техлемма 2:** пусть $x, y_1, y_2 \\in \\R^d$ и $\\cC_1, \\cC_2 \\subseteq \\R^d$ - замкнутые выпуклые множества. Определим $x_i^+ := \\Pi_{\\cC_i}(x - y_i)$. Имеет место два факта:\n",
    "\n",
    "1. Если $\\cC_2 = \\R^d$, то\n",
    "   $$\n",
    "       \\forall p \\in \\R^d\\ \\ \\|x_2^+ - p\\|^2 = \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2\n",
    "   $$\n",
    "   \n",
    "2. Если $\\cC_2 \\subseteq \\cC_1$, то\n",
    "   $$\n",
    "       \\begin{aligned}\n",
    "           \\forall p \\in \\cC_2\\ \\ \\|x_2^+ - p\\|^2 &\\le \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + 2\\tbr{y_2 - y_1, x_1^+ - x_2^+} - \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2\n",
    "           \\\\\n",
    "           &\\le \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + \\|y_2 - y_1\\|^2 - \\|x_1^+ - x\\|^2\n",
    "       \\end{aligned}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc1417",
   "metadata": {},
   "source": [
    "**Доказательство:**\n",
    "\n",
    "1. Воспользуемся той же схемой доказательства, что и в Техлемме 1: просто распишем левую часть:\n",
    "\n",
    "   $$\n",
    "       \\begin{aligned}\n",
    "           \\|x_2^+ - p\\|^2 &= \\|x_2^+ - x_1^+ + x_1^+ - x + x - p\\|^2\n",
    "           \\\\\n",
    "           &[\\text{через скал. произв.}]= \\|x_2^+ - x_1^+\\|^2 + \\|x_1^+ - x\\|^2 + \\|x - p\\|^2 + 2\\tbr{x_2^+ - x_1^+, x_1^+ - p} + 2\\tbr{x^+ - x, x - p}\n",
    "           \\\\\n",
    "           &[\\text{переход к проекции в конце}]= \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2 + \\|x - p\\|^2 + 2\\tbr{x_2^+ - x_1^+, x_1^+ - p} + 2\\tbr{x_1^+ - x, x_1^+ - p}\n",
    "           \\\\\n",
    "           &= \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2\n",
    "       \\end{aligned}\n",
    "   $$\n",
    "   \n",
    "   Равенство в 4й строке достигнуто за счёт того, что мы воспользовались преобразованием:\n",
    "   \n",
    "   $$\n",
    "       \\tbr{x^+ - x, x - p} = \\tbr{x^+ - x, x - x^+ + x^+ - p} = \\tbr{x^+ - x, x^+ - p} - \\|x^+ - x\\|^2\n",
    "   $$\n",
    "   \n",
    "   А последнее равенство достигнуто при помощи $x_2^+ - x = -y_2$, коль скоро $\\cC_2 = \\R^d$\n",
    "   \n",
    "2. За счёт вложенности множеств, имеем $x_2^+ \\in \\cC_2 \\subseteq \\cC_1$. Значит, мы можем воспользоваться результатом Техлеммы 1 с подстановкой $(x, y, x^+, p, \\cC) \\la (x, y_1, x_1^+, x_2^+ \\cC_1)$. Вместе с другой подстановкой $(x, y, x^+, p, \\cC) \\la (x, y_2, x_2^+, p, \\cC_2)$ вначале это даёт 2 неравенства:\n",
    "\n",
    "   $$\n",
    "       \\begin{aligned}\n",
    "           \\|x_1^+ - x_2^+\\|^2 &\\le \\|x - x_2^+\\|^2 - 2\\tbr{y_1, x_1^+ - x_2^+} - \\|x_1^+ - x\\|^2\n",
    "           \\\\\n",
    "           \\|x_2^+ - p\\|^2 &\\le \\|x - p\\|^2 - 2\\tbr{y_2, x_2^+ - p} - \\|x_2^+ - x\\|^2\n",
    "       \\end{aligned}\n",
    "   $$\n",
    "   \n",
    "   Если их сложить и перенести $\\|x_1^+ - x_2^+\\|^2$ в правую часть, то мы получаем в первое из неравенств пункта (по модулю того, что скалярные произведения нужно перегруппировать). Чтобы из него получить второе, нужно воспользоваться простеньким неравенством из неотрицательности нормы $\\|y_2 - y_1 + x_1^+ - x_2^+\\|^2 \\ge 0$:\n",
    "   \n",
    "   $$\n",
    "       2\\tbr{y_2 - y_1, x_1^+ - x_2^+} \\le \\|y_2 - y_1\\|^2 + \\|x_1^+ - x_2^+\\|^2\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4846c6",
   "metadata": {},
   "source": [
    "**Техлемма 3:** пусть $\\{a_t\\}_{t = 1}^\\infty \\subset \\R$ и $b, t_0 \\in \\N$ таковы, что выполнено рекурсивное неравенство:\n",
    "$$\n",
    "    \\exists q > 1, q' > 0 \\such \\forall t \\ge t_0\\; a_{t + 1} \\le \\ps{1 - \\frac{q}{t + b}}a_t + \\frac{q'}{(t + b)^2}\n",
    "$$\n",
    "Тогда рекурсивное неравенство можно развернуть в следующее:\n",
    "$$\n",
    "    a_t \\le \\frac{q'}{q - 1} \\cdot \\frac{1}{t} + o\\ps{\\frac{1}{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e9e58",
   "metadata": {},
   "source": [
    "**Доказательство:** для начала покажем, что мы можем к изучению такой $a'_t \\le a_t$, что выполнено то же неравенство из условия, но без крайнего слагаемого:\n",
    "$$\n",
    "    \\forall t \\ge t_0\\; a'_{t + 1} \\le \\ps{1 - \\frac{q}{t + b}}a'_t\n",
    "$$\n",
    "**Далее идёт слишком синтетическое утверждение, посему доказательство прерывается**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc5dc9",
   "metadata": {},
   "source": [
    "**Техлемма 4:** пусть $x^*$ - регулярное решение вариационного неравенства. Тогда, существуют константы $r, \\alpha, \\beta > 0$ такие, что выполнены свойства:\n",
    "- $V$ является $\\beta$-липшицевым на $K = \\ole{B}_r(x^*)$\n",
    "- $\\forall x \\in U := \\cX \\cap K\\ \\ \\tbr{V(x), x - x^*} \\ge \\alpha\\|x - x^*\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce63b4",
   "metadata": {},
   "source": [
    "**Доказательство:** существование окрестности, где $V$ является $\\beta$-липшицевым тривиально в силу того, что $V$ является $C^1$-гладкой функцией (а значит на компакте мы нужную константу найдём). Остаётся лишь найти $\\alpha$. Пусть $TC_\\cX(x^*)$ обозначает конус касательных направлений к $\\cX$ в точке $x^*$. Рассмотрим следующую функцию $\\phi \\colon \\R^{d \\times d} \\to \\R$:\n",
    "$$\n",
    "    \\phi(G) = \\min_{z \\in TC_\\cX(x^*) \\wedge \\|z\\| = 1} z^TGz\n",
    "$$\n",
    "Эта функция является выпуклой вверх, коль скоро представляет собой минимум от множества функций, линейных по тому же аргументу:\n",
    "$$\n",
    "    \\phi(\\theta G_1 + (1 - \\theta) G_2) = \\min_{z \\in TC_\\cX(x^*) \\wedge \\|z\\| = 1} \\underbrace{z^T(\\theta G_1 + (1 - \\theta) G_2)z}_{\\theta z^TG_1z + (1 - \\theta) z^TG_2z} \\ge \\theta \\phi(G_1) + (1 - \\theta) \\phi(G_2)\n",
    "$$\n",
    "В свою очередь, из-за этого факта $\\phi$ является непрерывной на внутренности своей эффективной области определения. В силу регулярности решения, $\\phi(J_V(x^*)) > 0$, а композиция $\\phi \\circ J_V$ непрерывна. Стало быть, можно найти $r, \\alpha > 0$ через поиск оценки $\\phi(J_V(x)) \\ge \\alpha$ для всех $x \\in U$. Итак, распишем разность значений $V$ через интеграл по якобиану (интеграл матрицы нужно понимать поэлементно):\n",
    "$$\n",
    "    V(x) - V(x^*) = \\ps{\\int_0^1 J_V\\big(x^* + \\lambda(x - x^*)\\big)d\\lambda}(x - x^*)\n",
    "$$\n",
    "Для упрощения вводим обозначения $z = x - x^* \\in TC_\\cX(x^*)$, $x_\\lambda = x^* + \\lambda(x - x^*) \\in K$. Тогда, взяв скалярное произведение с $z$ с обеих сторон, имеем:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\tbr{V(x) - V(x^*), z} &= \\tbr{V(x) - V(x^*), x - x^*} = z^T\\ps{\\int_0^1 J_V(x_\\lambda)d\\lambda}z\n",
    "        \\\\\n",
    "        &\\ge \\ps{\\int_0^1 \\phi(J_V(x_\\lambda))d\\lambda}\\|z\\|^2 \\ge \\alpha\\|z\\|^2 = \\alpha\\|x - x^*\\|^2\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Наконец, пользуемся тем, что $x^*$ является решением, а значит $\\tbr{V(x^*), x - x^*} \\ge 0$ и верна цепочка неравенств:\n",
    "$$\n",
    "    \\tbr{V(x), x - x^*} \\ge \\tbr{V(x) - V(x^*), x - x^*} \\ge \\alpha\\|x - x^*\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bd5f4",
   "metadata": {},
   "source": [
    "### Анализ детерминированного случая"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfbac2",
   "metadata": {},
   "source": [
    "Как и было обговорено ранее, мы считаем детерминированный случай просто вырожденным случаем для стохастического, где $\\sigma = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f97028",
   "metadata": {},
   "source": [
    "#### Эргодическое среднее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13e8aa",
   "metadata": {},
   "source": [
    "**Теорема о глобальной сходимости (для эргодического среднего):** пусть вариационная задача удовлетворяет условиям:\n",
    "1. $\\cX^* \\neq \\emptyset$ - множество решений вариационного неравенства непусто\n",
    "2. $V$ является $\\beta$-липшицевым оператором\n",
    "3. $V$ является монотонным оператором\n",
    "\n",
    "Если рассмотреть 1-EG метод с постоянным шагом $\\gamma < 1 / (c\\beta)$ (где $c = 1 + \\sqrt{2}$ для RG и $c = 2$ для PEG, OG). Тогда имеет место неравенство:\n",
    "\n",
    "$$\n",
    "    \\forall R > 0\\ \\ \\Err_R\\big(\\ole{X}_t\\big) \\le \\frac{1}{t} \\cdot \\frac{R^2 + \\|X_1 - X_{1 / 2}\\|^2}{2\\gamma}\n",
    "$$\n",
    "\n",
    "где в определении $\\Err_R$ берётся $\\cX_R = \\cX \\cap \\ole{B}_R(X_1)$, а также $\\ole{X}_t = \\frac{1}{t}\\sum_{s = 1}^t X_{s + 1 / 2}$ - эргодическое среднее по направляющим точкам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01d9f0",
   "metadata": {},
   "source": [
    "**Лемма:** если $V$ является монотонным оператором и удовлетворяет следующему неравенству:\n",
    "$$\n",
    "    \\exists \\mu_s, \\lambda_s \\ge 0 \\such \\forall p \\in \\cX_R, s \\in \\range{1}{t}\\; \\|X_{s + 1} - p\\|^2 \\le \\|X_s - p\\|^2 - 2\\lambda_s\\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} + \\mu_s - \\mu_{s + 1}\n",
    "$$\n",
    "где $\\cX_R = \\cX \\cap \\ole{B}_R(X_1)$. Тогда имеет место следующее неравенство для ошибки:\n",
    "$$\n",
    "    \\Err_R \\ps{\\ole{X}} \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$\n",
    "где $\\ole{X} = \\frac{\\sum_{s = 1}^t \\lambda_sX_{s + 1 / 2}}{\\sum_{s = 1}^t \\lambda_s}$ - средневзвешенное ведущих точек $X_{s + 1 / 2}$ по весам $\\lambda_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7a298",
   "metadata": {},
   "source": [
    "**Доказательство леммы:** как и во многих других случаях с эргодическим средним, мы будем делать телескопическую сумму, в чём нам особенно хорошо помогает тот объект, который дан в условии. Перенесём скалярное произведение в левую сторону и просуммируем:\n",
    "\n",
    "$$\n",
    "    \\sum_{s = 1}^t 2\\lambda_s\\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\le \\|X_1 - p\\|^2 - \\|X_{t + 1} - p\\|^2 + \\mu_1 - \\mu_{t + 1} \\le \\|X_1 - p\\|^2 + \\mu_1\n",
    "$$\n",
    "\n",
    "Согласно условию, $p \\in \\cX_R = \\cX \\cap \\ole{B}_R(X_1)$, а значит автоматически $\\|X_1 - p\\|^2 \\le R^2$. Более того, по монотонности $V$ мы можем преобразовать сумму слева:\n",
    "\n",
    "$$\n",
    "    \\tbr{V(X_{s + 1 / 2}) - V(p), X_{s + 1 / 2} - p} \\ge 0 \\Lora \\tbr{V(p), X_{s + 1 / 2} - p} \\le \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p}\n",
    "$$\n",
    "\n",
    "Теперь соберём всё вместе:\n",
    "\n",
    "$$\n",
    "    \\sum_{s = 1}^t 2\\lambda_s\\tbr{V(p), X_{s + 1 / 2} - p} \\le R^2 + \\mu_1\n",
    "$$\n",
    "\n",
    "Поделим обе стороны на 2, занесём сумму под скалярное произведение, а также поделим на $\\sum_{s = 1}^t \\lambda_s$:\n",
    "\n",
    "$$\n",
    "    \\tbr{V(p), \\frac{\\sum_{s = 1}^t \\lambda_sX_{s + 1 / 2}}{\\sum_{s = 1}^t \\lambda_s} - p} \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$\n",
    "\n",
    "Максимизация по $p$ приводит значение левой части к значению $\\Err_R$ с соответствующим аргументом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7d0c4",
   "metadata": {},
   "source": [
    "**Доказательство теоремы:** вся идея состоит в том, чтобы свести по отдельности PEG, OG и RG к доказанной лемме. Нужно отметить, что мы будем требовать для PEG и OG инициализацию с любым $X_{1 / 2}$ и $X_1 \\in \\cX$, в то время как для RG всё начиинается точек $X_0$ и $X_{1 / 2}$. К сожалению, каждый метод нужно сводить отдельно:\n",
    "\n",
    "- **PEG.** \n",
    "  1. (Подстановка параметров во 2е неравенство Техлеммы 2, использование липшцевости $V$) При $t \\ge 1$ мы можем воспользоваться неравенствами Техлеммы 2 с подстановкой $(x, y_1, y_2, x_1^+, x_2^+, \\cC_1, \\cC_2) \\la (X_t, \\gamma V(X_{t - 1 / 2}), \\gamma V(X_{t + 1 / 2}), X_{t + 1 / 2}, X_{t + 1}, \\cX, \\cX)$:\n",
    "  \n",
    "  $$\n",
    "      \\|X_{t + 1} - p\\|^2 \\le \\|X_t - p\\|^2 - 2\\gamma\\tbr{V(X_{t + 1 / 2}), X_{t + 1 / 2} - p} + \\gamma^2\\|V(X_{t + 1 / 2}) - V(X_{t - 1 / 2})\\|^2 - \\|X_{t + 1 / 2} - X_t\\|^2\n",
    "  $$\n",
    "  \n",
    "  Можем перейти от нормы разности с $V$ к норме разности аргументов в силу $\\beta$-липшицевости:\n",
    "  $$\n",
    "      \\|X_{t + 1} - p\\|^2 \\le \\|X_t - p\\|^2 - 2\\gamma\\tbr{V(X_{t + 1 / 2}), X_{t + 1 / 2} - p} + \\gamma^2\\beta^2\\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2 - \\|X_{t + 1 / 2} - X_t\\|^2\n",
    "  $$\n",
    "  \n",
    "  2. (За счёт неравенства Юнга и свойства нерасширяемости проекции получить неравенство на норму одного слагаемого из первого пункта) Сведём последние 2 слагаемых так, чтобы получить $\\mu_s - \\mu_{s + 1}$ согласно лемме. Идея в том, что предпоследнее слагаемое можно оценить так, что при заданной оценке $\\gamma < 1/(2\\beta)$ оно уберёт последнее, останется только нужная разность каких-то величин. Итак, по следствию неравенства Юнга:\n",
    "  \n",
    "  $$\n",
    "      \\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2 \\le 2\\|X_{t + 1 / 2} - X_t\\|^2 + 2\\|X_t - X_{t - 1 / 2}\\|^2\n",
    "  $$\n",
    "  \n",
    "  Через свойство нерасширяемости проекции оцениваем первую норму (это работает только при $t \\ge 2$! Случай $t = 1$ разберём отдельно):\n",
    "  \n",
    "  $$\n",
    "      \\begin{aligned}\n",
    "          \\|X_t - X_{t - 1 / 2}\\|^2 &\\le \\Big\\|\\big(X_{t - 1} - \\gamma V(X_{t - 1 / 2})\\big) - \\big(X_{t - 1} - \\gamma V(X_{t - 3 / 2})\\big)\\Big\\|^2 = \\gamma^2\\|V(X_{t - 3 / 2}) - V(X_{t - 1 / 2})\\|^2\n",
    "          \\\\\n",
    "          &\\le [\\text{Lipschitz}] \\le \\gamma^2\\beta^2\\|X_{t - 1 / 2} - X_{t - 3 / 2}\\|^2\n",
    "      \\end{aligned}\n",
    "  $$\n",
    "  \n",
    "  Теперь соберём всё вместе *и учтём, что по условию $\\gamma\\beta < 1 / 2$*. Маленькое наравенство:\n",
    "  \n",
    "  $$\n",
    "      \\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2 \\le 2\\|X_{t + 1 / 2} - X_t\\|^2 + \\frac{1}{2}\\|X_{t - 1 / 2} - X_{t - 3 / 2}\\|^2\n",
    "  $$\n",
    "  \n",
    "  Если мы подставим это выражение в таком виде в основное неравенство, то заветного сокращения не будет. Схитрим, ведь $a = 2a - a$:\n",
    "  \n",
    "  $$\n",
    "      \\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2 \\le 4\\|X_{t + 1 / 2} - X_t\\|^2 + \\|X_{t - 1 / 2} - X_{t - 3 / 2}\\|^2 - \\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2\n",
    "  $$\n",
    "  \n",
    "  Ну и теперь подстановка в главное неравенство ($4\\gamma^2\\beta^2 < 4 \\cdot (1 / 4) = 1$):\n",
    "  \n",
    "  $$\n",
    "      \\begin{aligned}\n",
    "          \\|X_{t + 1} - p\\|^2 &\\le \\|X_t - p\\|^2 - 2\\gamma\\tbr{V(X_{t + 1 / 2}), X_{t + 1 / 2} - p}\n",
    "          \\\\\n",
    "          &+ \\gamma^2\\beta^2\\Big(4\\|X_{t + 1 / 2} - X_t\\|^2 + \\|X_{t - 1 / 2} - X_{t - 3 / 2}\\|^2 - \\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2\\Big) - \\|X_{t + 1 / 2} - X_t\\|^2\n",
    "          \\\\\n",
    "          &\\le \\|X_t - p\\|^2 - 2\\gamma\\tbr{V(X_{t + 1 / 2}), X_{t + 1 / 2} - p} + \\underbrace{\\gamma^2\\beta^2\\|X_{t - 1 / 2} - X_{t - 3 / 2}\\|^2}_{\\mu_t} - \\underbrace{\\gamma^2\\beta^2\\|X_{t + 1 / 2} - X_{t - 1 / 2}\\|^2}_{\\mu_{t + 1}}\n",
    "      \\end{aligned}\n",
    "  $$\n",
    "  \n",
    "  При указанных $\\mu_t$ и $\\lambda_t = \\gamma$, получаем выполнение условий леммы при $t \\ge 2$. Случай $t = 1$ нужно разобрать, ибо там участвует уже определённое $\\mu_2$, а значит надо корректно определить $\\mu_1$. Берём эти последние 2 слагаемых и действуем уже имеющимися неравенствами:\n",
    "  \n",
    "  $$\n",
    "      \\begin{aligned}\n",
    "          \\gamma^2\\beta^2\\|X_{3 / 2} - X_{1 / 2}\\|^2 - \\|X_{3 / 2} - X_1\\|^2 &\\le [\\text{Young and trick}]\n",
    "          \\\\\n",
    "          &\\le \\gamma^2\\beta^2\\Big(2\\big(2\\|X_{3 / 2} - X_1\\|^2 + 2\\|X_1 - X_{1 / 2}\\|^2\\big) - \\|X_{3 / 2} - X_{1 / 2}\\|^2\\Big) - \\|X_{3 / 2} - X_1\\|^2\n",
    "          \\\\\n",
    "          &\\le \\underbrace{4\\gamma^2\\beta^2\\|X_1 - X_{1 / 2}\\|^2}_{\\mu_1} - \\underbrace{\\gamma^2\\beta^2\\|X_{3 / 2} - X_{1 / 2}\\|^2}_{\\mu_2}\n",
    "      \\end{aligned}\n",
    "  $$\n",
    "  \n",
    "  Отсюда $\\mu_1 = 4\\gamma^2\\beta^2\\|X_1 - X_{1 / 2}\\|^2 \\le \\|X_1 - X_{1 / 2}\\|^2$. Подстановка всех величин в лемму даёт искомый результат.\n",
    "\n",
    "- **OG.**\n",
    "  1. Подстановка параметров в 1е неравенство Техлеммы 2\n",
    "  2. Подставив неравенства из свойства разделения проекции, липшицевости $V$ и того же неравенства Юнга в первый пункт, получить искомое неравенство\n",
    "\n",
    "- **RG.**\n",
    "  1. Подстановка параметров в 2е неравенство Техлеммы 2\n",
    "  2. Увидеть 2 неравенства из свойства разделения проекции, просуммировать их.\n",
    "  3. Сделать оценку на единственное скалярное произведение из пункта 1, собрать это воедино\n",
    "  4. Дважды неравенство Юнга для произведения модулей из третьего пункта. Собрать воедино"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545b646",
   "metadata": {},
   "source": [
    "**Улучшение леммы в задачах минизации и min-max**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c09592",
   "metadata": {},
   "source": [
    "Лемма 2 может быть улучшена для конкретных задач: получаются другие функции ошибок, которые могут лучше отражать специфику задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d1553",
   "metadata": {},
   "source": [
    "**Задача минимизации:** $V = \\nabla f$. В силу монотонности $V$ по условию, сама $f$ является выпуклой. Отсюда для любой точки $p \\in \\cX$ по неравенству Йенсена получаем:\n",
    "\n",
    "$$\n",
    "    \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} = \\tbr{\\nabla f(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\ge f(X_{s + 1 / 2}) - f(p)\n",
    "$$\n",
    "\n",
    "Объединив это с рассматриваемым средневзвешенным в лемме, получим следующее:\n",
    "\n",
    "$$\n",
    "    \\frac{1}{\\sum_{s = 1}^t \\lambda_s} \\sum_{s = 1}^t \\lambda_s \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\ge \\frac{1}{\\sum_{s = 1}^t \\lambda_s} \\sum_{s = 1}^t \\lambda_s f(X_{s + 1 / 2}) - f(p) \\ge f(\\ole{X}) - f(p)\n",
    "$$\n",
    "\n",
    "Применим это неравенство к $p \\in \\cX^*$. Тогда, используя первое неравенство из доказательства леммы с $R = \\rho(X_1, \\cX^*)$, мы получаем следующий результат:\n",
    "\n",
    "$$\n",
    "    f(\\ole{X}) - f^* \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a13411",
   "metadata": {},
   "source": [
    "**Задача min-max:** $V = (\\nabla_\\theta \\cL, -\\nabla_\\phi \\cL)$. Монотонность такой $V$ эквивалентна тому, что $\\cL$ является выпуклой по $\\theta$ и вогнутой по $\\phi$. Обозначим $X_{s + 1 / 2} = (\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2})$ и $p = (\\theta, \\phi)$. За счёт уже упомянутой выпуклости-вогнутости $\\cL$, мы получаем следующее неравенство:\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} &= \\tbr{\\nabla_\\theta \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2}), \\theta_{s + 1 / 2} - \\theta} - \\tbr{\\nabla_\\phi \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2}), \\phi_{s + 1 / 2} - \\phi}\n",
    "        \\\\\n",
    "        &\\ge \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2}) - \\cL(\\theta, \\phi_{s + 1 / 2}) + \\cL(\\theta_{s + 1 / 2}, \\phi) - \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2})\n",
    "        \\\\\n",
    "        &= \\cL(\\theta_{s + 1 / 2}, \\phi) - \\cL(\\theta, \\phi_{s + 1 / 2})\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "Как и ранее, теперь мы объединяем это со средневзвешенным (тут $\\ole{X} = (\\ole{\\theta}, \\ole{\\phi})$):\n",
    "\n",
    "$$\n",
    "    \\frac{1}{\\sum_{s = 1}^t \\lambda_s} \\sum_{s = 1}^t \\lambda_s \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\ge \\cL(\\ole{\\theta}, \\phi) - \\cL(\\theta, \\ole{\\phi})\n",
    "$$\n",
    "\n",
    "Снова подставляем полученный результат в первое неравенство леммы. Максимизация по $p = (\\theta, \\phi) \\in \\cX \\cap \\ole{B}_R(X_1)$ даёт следующую оценку:\n",
    "\n",
    "$$\n",
    "    NI_R(\\ole{X}) \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaed74f",
   "metadata": {},
   "source": [
    "#### Последняя итерация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141b7ef",
   "metadata": {},
   "source": [
    "\n",
    "**Теорема о глобальной сходимости (для последней итерации в монотонном случае):** пусть вариационная задача удовлетворяет условиям:\n",
    "1. $\\cX^* \\neq \\emptyset$ - множество решений вариационного неравенства непусто\n",
    "2. $V$ является $\\beta$-липшицевым оператором\n",
    "3. $V$ является $\\alpha$-строго монотонным оператором\n",
    "\n",
    "Пусть также $x^*$ - единственное решение вариационного неравенства. Тогда, если 1-EG метод запущен с постоянным достаточно малым шагом $\\gamma$, то полученная последовательность точек $X_t$ сходится к $x^*$ со следущей асимптотикой:\n",
    "$$\n",
    "    \\exists \\rho > 0 \\such \\|X_t - x^*\\| = O\\big(\\exp(-\\rho t)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13e82e",
   "metadata": {},
   "source": [
    "**Теорема о локальной сходимости (для последней итерации в немонотонном случае):** пусть $x^*$ - регулярное решение вариационного неравенства. Тогда, если рассмотреть 1-EG метод, который инициализирован достаточно близко к $x^*$ и имеет достаточно малый шаг $\\gamma$, то полученная последовательность точек $X_t$ сходится к $x^*$ со следующей асимптотикой:\n",
    "$$\n",
    "    \\exists \\rho > 0 \\such \\|X_t - x^*\\| = O\\big(\\exp(-\\rho t)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125b5db",
   "metadata": {},
   "source": [
    "**Доказательство немонотонного случая для PEG и OG:** согласно Техлемме 4, существуют такие константы $r, \\alpha, \\beta > 0$, что выполнены условия:\n",
    "- $V$ является $\\beta$-липшицевым на $K = \\ole{B}_r(x^*)$\n",
    "- $\\forall x \\in U := \\cX \\cap K\\ \\ \\tbr{V(x), x - x^*} \\ge \\alpha\\|x - x^*\\|^2$\n",
    "\n",
    "Из второго факта на самом деле следует, что если мы покажем принадлежность $X_t \\in K$ при всех $t \\in \\N / 2$, то всё будет доказано (случай просто сведётся к монотонному, где ключевым фактом служит как раз аналогичное неравенство $\\tbr{V(X_{t + 1 / 2}), X_{t + 1 / 2} - x^*} \\ge \\alpha\\|X_{t + 1 / 2} - x^*\\|^2$). Итак, чтобы показать требуемое, мы найдём такие начальные условия, что все точки попадут под результат Техлеммы 4:\n",
    "1. $\\forall t \\in \\N\\ \\ \\|X_t - x^*\\| \\le \\frac{r^2}{4}$\n",
    "2. $\\forall t \\in \\N\\ \\ X_{t + 1 / 2} \\in K$\n",
    "\n",
    "Мы найдём такие начальные значения, что факты выше будут выполнены индуктивно.\n",
    "1. Покажем как делать индукционный переход для $t > 1$ в предположении, что при всех $s \\in \\N \\wedge s \\le t$ у нас есть  принадлежность $X_s \\in K$. Тогда в случаях PEG и OG $X_{s + 1 / 2} \\in \\cX \\cap K = U$ и у нас выполнено неравенство $\\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - x^*} \\ge 0$. За счёт этой оценки, мы напрямую из неравенства леммы можем получить такую связь:\n",
    "\n",
    "   $$\n",
    "       \\|X_t - x^*\\|^2 + \\mu_t \\le \\|X_{t - 1} - x^*\\|^2 + \\mu_{t - 1} \\le \\ldots \\le \\|X_1 - x^*\\|^2 + \\mu_1\n",
    "   $$\n",
    "\n",
    "   В силу того, как мы выбираем начальные условия в PEG и OG, мы можем потребовать $X_{1 / 2} = X_1$ и тогда $\\mu_1 = 0$, а подобрать $X_1$ так, чтобы выполнить неравенство $\\|X_1 - x^*\\|^2 \\le \\frac{r^2}{4}$ мы уже как-нибудь сможем.\n",
    "   \n",
    "2. Теперь покажем, что мы сможем подобрать достаточно малый шаг $\\gamma$ такой, что $\\|X_t - x^*\\|^2 \\le \\frac{r^2}{4}$ и $X_{t - 1 / 2} \\in K$, то и $X_{t + 1 / 2} \\in K$. В силу нерасширяемости проекции, мы имеем право записать следующее:\n",
    "\n",
    "   $$\n",
    "       \\|X_{t + 1 / 2} - X_t\\|^2 \\le \\|X_t - \\gamma V(X_{t - 1 / 2}) - X_t\\|^2 = \\gamma^2\\|V(X_{t - 1 / 2})\\|^2\n",
    "   $$\n",
    "\n",
    "   В силу непрерывности $V$ на компактности $K$, оператор ограничен. Обозначим за $M$ соответствующую константу. Тогда мы можем потребовать $\\gamma < r / (2M)$, откуда получим\n",
    "\n",
    "   $$\n",
    "       X_{t - 1 / 2} \\in K \\Ra \\gamma^2\\|V(X_{t - 1 / 2})\\|^2 \\le \\frac{r^2}{4M^2} \\cdot M^2 = \\frac{r^2}{4}\n",
    "   $$\n",
    "   \n",
    "   Остаётся воспользоваться неравенством Юнга и получить требуемое:\n",
    "   \n",
    "   $$\n",
    "       \\|X_{t + 1 / 2} - x^*\\|^2 \\le 2\\|X_{t + 1 / 2} - X_t\\|^2 + 2\\|X_t - x^*\\|^2 \\le r^2 \\Ra X_{t + 1 / 2} \\in K\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767a969",
   "metadata": {},
   "source": [
    "### Обзор стохастического случая"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb331032",
   "metadata": {},
   "source": [
    "**Теорема о глобальной сходимости:** пусть вариационная задача удовлетворяет условиям:\n",
    "1. $\\cX^* \\neq \\emptyset$ - множество решений вариационного неравенства непусто\n",
    "2. $V$ является $\\beta$-липшицевым оператором\n",
    "3. $V$ является $\\alpha$-строго монотонным оператором\n",
    "\n",
    "Если рассмотреть PEG-алгоритм со стохастическим оракулом, чьи ответы удовлетворяют модели, а также шаг имеет вид $\\gamma_t = \\gamma / (t + b)$ для некоторых $\\gamma > 1 / \\alpha$ и $b \\ge 4\\beta\\gamma$, то имеет место неравенство:\n",
    "$$\n",
    "    \\E\\big(\\|X_t - x^*\\|^2\\big) \\le \\frac{6\\gamma^2\\sigma^2}{\\alpha\\gamma - 1} \\cdot \\frac{1}{t} + o\\ps{\\frac{1}{t}}\n",
    "$$\n",
    "При этом, для эргодического среднего $\\ole{X}_t = \\frac{1}{t}\\sum_{s = 1}^t X_s$ тоже верно похожее неравенство:\n",
    "$$\n",
    "    \\E\\big(\\|\\ole{X}_t - x^*\\|^2\\big) \\le \\frac{6\\gamma^2\\sigma^2}{\\alpha\\gamma - 1} \\cdot \\frac{\\log t}{t} + o\\ps{\\frac{\\log t}{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3b441",
   "metadata": {},
   "source": [
    "**Теорема о локальной сходимости:** пусть $x^*$ - регулярное решение вариационного неравенства, $\\delta > 0$ - *уровень терпимости*. Если рассмотреть PEG-алгоритм со стохастическим оракулом, чьи ответы удовлетворяют описанной модели, а также шаг имеет вид $\\gamma_t = \\gamma / (t + b)$ для некоторых $\\gamma > 1 / \\alpha$ и достаточно большого $b$, то имеют место утверждения:\n",
    "1. Существуют окрестности $U, U_1 \\subseteq \\cX$ около точки $x^*$ такие, что если $X_{1 / 2} \\in U$ и $X_1 \\in U_1$, то событие $E_\\infty = \\{\\forall t \\in \\N\\ \\ X_{t + 1 / 2} \\in U\\}$  (все ведущие точки лежат в окрестности $U$) происходит с вероятностью не менее $1 - \\delta$.\n",
    "\n",
    "2. В предположении первого пункта, имеет место неравенство:\n",
    "$$\n",
    "    \\E\\big(\\|X_t - x^*\\|^2 | E_\\infty\\big) \\le \\frac{4\\gamma^2(M^2 + \\sigma^2)}{(\\alpha\\gamma - 1)(1 - \\delta)} \\cdot \\frac{1}{t} + o\\ps{\\frac{1}{t}}\n",
    "$$\n",
    "где $M = \\sup_{x \\in U} \\|V(x)\\| < \\infty$ и $\\alpha = \\inf_{x \\in U} \\frac{\\tbr{V(x), x - x^*}}{\\|x - x^*\\|^2} > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4725a3",
   "metadata": {},
   "source": [
    "**Замечание к теореме:** конечность $M$ и положительность $\\alpha$ являются следствием регулярности $x^*$, а их значения зависят только на величине окрестности $U$. Так, с ростом окрестности $U$, которая отвечает за область почти достоверной сходимости, мы получаем более плохую сходимость, ибо $M$ неубывает с этим ростом, а $\\alpha$ может только уменьшится (это плохо, ибо оно в знаменателе). Схожим образом $U_1$ зависит только от $U$, причём имеет смысл рассматривать такие $U_1$, что они составляют четверть от $U$ (в смысле меры)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117e50b",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450ecc6",
   "metadata": {},
   "source": [
    "### Цели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249c066",
   "metadata": {},
   "source": [
    "1. Показать работу экстра-градиентных методов для решения задачи вариационного неравенства\n",
    "2. Показать, что рассмотренные 1-EG методы показывают себя качественно и количественно лучше чем стандартный экстраградиентный метод\n",
    "3. Попробовать выбрать лучший из предложенных методов\n",
    "4. Подтвердить результаты статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f7f18",
   "metadata": {},
   "source": [
    "Рассмотрим min-max задачу без ограничений для функции $\\cL(\\theta, \\phi)$ следующего вида:\n",
    "$$\n",
    "    \\cL(\\theta, \\phi) = 2\\eps_1(\\theta^TA_1\\theta) + \\eps_2(\\theta^TA_2\\theta)^2 - 2\\eps_1(\\phi^TB_1\\phi) - \\eps_2(\\phi^TB_2\\phi)^2 + 4(\\theta^TC\\phi)\n",
    "$$\n",
    "Пусть $\\cX = \\R^d = \\R^{d_1} \\times \\R^{d_2}$ для $d_1 = d_2 = 1000$, а также $A_i, B_i \\succ 0$.\n",
    "\n",
    "Мы рассмотрим 3 конкретных случая, для простоты $C = 0_d$:\n",
    "\n",
    "1. $\\eps_1 = 1$, $\\eps_2 = 0$. В этом случае $\\cL(\\theta, \\phi) = 2\\theta^TA_1\\theta - 2\\phi^TB_1\\phi + 4\\theta^TC\\phi$. Оператор $V$ имеет следующий вид:\n",
    "\n",
    "$$\n",
    "    V(\\theta, \\phi) = (\\nabla_\\theta \\cL, -\\nabla_\\phi \\cL) = 4(A_1\\theta, B_1\\phi)\n",
    "$$\n",
    "\n",
    "Покажем, что он является строго монотонным. Воспользуемся определением:\n",
    "\n",
    "$$\n",
    "    \\tbr{V(x') - V(x), x' - x} \\ge \\alpha\\|x' - x\\|^2 = \\alpha\\tbr{x' - x, x' - x}\n",
    "$$\n",
    "\n",
    "Перенесём всё в левую часть и подставим $V$:\n",
    "\n",
    "$$\n",
    "    \\tbr{((4A_1 - \\alpha E_d)(\\theta' - \\theta), (4B_1 - \\alpha E_d)(\\phi' - \\phi)), x' - x} \\ge 0\n",
    "$$\n",
    "\n",
    "Скалярное произведение можно разбить на два в силу определения:\n",
    "\n",
    "$$\n",
    "    \\tbr{(4A_1 - \\alpha E_d)(\\theta' - \\theta), \\theta' - \\theta} + \\tbr{(4B_1 - \\alpha E_d)(\\phi' - \\phi), \\phi' - \\phi} \\ge 0\n",
    "$$\n",
    "\n",
    "Скалярные произведения не зависят друг от друга. Несложно увидеть, что если хоть одно из них меньше нуля, то можно устремить всё в $-\\infty$. Стало быть, мы хотим, чтобы и то, и другое было неотрицательно. Это выполняется тогда и только тогда, когда соответствующие матрицы неотрицательно определены:\n",
    "\n",
    "$$\n",
    "    \\alpha \\colon \\System{\n",
    "        &{4A_1 - \\alpha E_d \\succeq 0 \\Lra \\alpha \\le 4\\lambda_\\min(A_1)}\n",
    "        \\\\\n",
    "        &{4B_1 - \\alpha E_d \\succeq 0 \\Lra \\alpha \\le 4\\lambda_\\min(B_1)}\n",
    "    }\n",
    "$$\n",
    "\n",
    "Отсюда $\\alpha \\le 4\\min\\{\\lambda_\\min(A_1), \\lambda_\\min(B_1)\\}$\n",
    "\n",
    "2. $\\eps_1 = 0$, $\\eps_2 = 1$. В такой ситуации $\\cL(\\theta, \\phi) = (\\theta^TA_2\\theta)^2 - (\\phi^TB_2\\phi)^2$. Оператор $V$ принимает вид:\n",
    "\n",
    "$$\n",
    "    V(\\theta, \\phi) = 8\\big((\\theta^TA_2\\theta)A_2\\theta, (\\phi^TB_2\\phi)B_2\\phi\\big) = 8(A_2\\theta\\theta^TA_2\\theta, B_2\\phi\\phi^TB_2\\phi)\n",
    "$$\n",
    "\n",
    "Покажем, что $V$ монотонна:\n",
    "\n",
    "$$\n",
    "    V(x') - V(x) = 8\\big(A_2(\\theta'\\theta'^TA_2\\theta' - \\theta\\theta^TA_2\\theta), B_2(\\phi'\\phi'^TB_2\\phi' - \\phi\\phi^TB_2\\phi)\\big)\n",
    "$$\n",
    "\n",
    "**ДОПИСАТЬ**\n",
    "\n",
    "3. $\\eps_1 = 1 = -\\eps_2$, случайный оракул с ошибкой $Z \\sim N(0, 10^{-2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc502643",
   "metadata": {},
   "source": [
    "### Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74f8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, hessian, jit, vmap\n",
    "#from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a297373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax import config\n",
    "# config.update(\"jax_enable_x64\", True) # for float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12e21f",
   "metadata": {},
   "source": [
    "Постановка задачи в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c58fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_possymm_matrix(d, m, L):\n",
    "    B = jnp.diag(np.random.rand(d) * (L - m) + m)\n",
    "    Q, _ = jnp.linalg.qr(np.random.rand(d, d))\n",
    "    return Q.T @ B @ Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e7c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1580)\n",
    "d = 1000\n",
    "sigma = 0.1\n",
    "coefs_arr = [(1, 0), (0, 1), (1, -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ff25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = jnp.array([-5, -4, -3, -2])\n",
    "Ls = jnp.array([-4, 5, 4, -1])\n",
    "\n",
    "C = jnp.zeros(shape=(d, d))\n",
    "Mtxs = jnp.zeros(shape=(len(ms), d, d))\n",
    "\n",
    "for i, (m, L) in enumerate(zip(ms, Ls)):\n",
    "    Mtxs[i] = generate_possymm_matrix(d, m, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d268c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_L_func(theta, phi, e1, e2, A1, A2, B1, B2, C):\n",
    "    s1 = 2 * e1 * theta.T @ A1 @ theta\n",
    "    s2 = e2 * (theta @ A2 @ theta.T) ** 2\n",
    "    s3 = 2 * e1 * phi.T @ B1 @ phi\n",
    "    s4 = e2 * (phi @ B2 @ phi.T) ** 2\n",
    "    s5 = 4 * theta @ C @ phi.T\n",
    "    return s1 + s2 - s3 - s4 + s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2db7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_funcs = np.array([\n",
    "    jit(lambda theta, phi: general_L_func(theta,\n",
    "                                          phi,\n",
    "                                          *coefs,\n",
    "                                          *Mtxs, C)) for coefs in coefs_arr\n",
    "], dtype='O')\n",
    "\n",
    "L_grads = np.array([\n",
    "    grad(L_func) for L_func in L_funcs\n",
    "], dtype='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af3b62",
   "metadata": {},
   "source": [
    "Реализация методов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_gradient_descent(L_func, oracle_func, gamma_func, x_0, x_sol, R, K=10**3, mode='x_k+1 - x_k'):\n",
    "    '''\n",
    "        L_func - целевая функция (нужна для подсчёта ошибки)\n",
    "        oracle_func - функция оракула\n",
    "        gamma_func - функция шага\n",
    "        x_0 - стартовая точка\n",
    "        x_sol - точное решение (нужно для подсчёта ошибки)\n",
    "        R - радиус значимых точек (нужно для подсчёта ошибки с режимами 'Err' и 'NI')\n",
    "        K - количество итераций (по умолчанию 1е3)\n",
    "        mode - критерий сходимости\n",
    "                'x_k - x^*' - тогда критерий сходимости будет ||x_k - x^*||_2,\n",
    "               'f(x_k) - f(x^*)' - тогда критерий сходимости будет L_func(x_k) - L_func(x^*),\n",
    "               'x_k+1 - x_k' - аналогично первому\n",
    "               'f(x_k+1) - f(x_k)' - аналогично второму\n",
    "               'Err' - функция ошибки вариационных неравенств: max <V(x), x_k - x>, где ||x||_2 <= R\n",
    "               'NI' - функция ошибки Никаидо-Исоды: max L(theta_k, phi) - min L(theta, phi_k),\n",
    "                       где ||phi||_2 <= R и ||phi_k||_2 <= R\n",
    "                       \n",
    "       Функция возвращает седловую точку и вектор ошибок.\n",
    "    '''\n",
    "    errors = np.zeros(shape=(K + 1))\n",
    "    xs = np.zeros(shape=(2 * K + 1))\n",
    "    xs[0] = x_0\n",
    "    \n",
    "    for t in jnp.arange(1, K + 1):\n",
    "        gamma = gamma_func(t)\n",
    "        x = xs[2 * (t - 1)]\n",
    "        xs[2 * t - 1] = leading_x = x - gamma * oracle_func(x)\n",
    "        xs[2 * t] = x - gamma * oracle_func(leading_x)\n",
    "    \n",
    "    return \n",
    "    \n",
    "def past_extra_gradient_descent(L_func, oracle_func, gamma_func, x_0, x_sol, R, K=10**3, mode='x_k+1 - x_k'):\n",
    "    '''\n",
    "        L_func - целевая функция (нужна для подсчёта ошибки)\n",
    "        oracle_func - функция оракула\n",
    "        gamma_func - функция шага\n",
    "        x_0 - стартовая точка\n",
    "        x_sol - точное решение (нужно для подсчёта ошибки)\n",
    "        R - радиус значимых точек (нужно для подсчёта ошибки с режимами 'Err' и 'NI')\n",
    "        K - количество итераций (по умолчанию 1е3)\n",
    "        mode - критерий сходимости\n",
    "                'x_k - x^*' - тогда критерий сходимости будет ||x_k - x^*||_2,\n",
    "               'f(x_k) - f(x^*)' - тогда критерий сходимости будет L_func(x_k) - L_func(x^*),\n",
    "               'x_k+1 - x_k' - аналогично первому\n",
    "               'f(x_k+1) - f(x_k)' - аналогично второму\n",
    "               'Err' - функция ошибки вариационных неравенств: max <V(x), x_k - x>, где ||x||_2 <= R\n",
    "               'NI' - функция ошибки Никаидо-Исоды: max L(theta_k, phi) - min L(theta, phi_k),\n",
    "                       где ||phi||_2 <= R и ||phi_k||_2 <= R\n",
    "                       \n",
    "        Функция возвращает седловую точку и вектор ошибок.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e175b",
   "metadata": {},
   "source": [
    "## Затронутая при анализе литература\n",
    "\n",
    "1. https://arxiv.org/abs/1908.08465\n",
    "2. https://arxiv.org/abs/2006.08141"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daEnv",
   "language": "python",
   "name": "daenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

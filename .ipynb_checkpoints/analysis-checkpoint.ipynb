{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25ba226",
   "metadata": {},
   "source": [
    "# Анализ статьи [On the convergence of Single-Call Stochastic Extra-Gradient Methods](https://arxiv.org/abs/1908.08465)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c3e367",
   "metadata": {},
   "source": [
    "**Latex preset**\n",
    "$\\require{amsmath}$\n",
    "\n",
    "$\\newcommand{\\eps}{\\varepsilon}$\n",
    "$\\newcommand{\\bfone}{\\mathbf{1}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\N}{\\mathbb{N}}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\bS}{\\mathbb{S}}$\n",
    "$\\newcommand{\\cC}{\\mathcal{C}}$\n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$\n",
    "$\\newcommand{\\cF}{\\mathcal{F}}$\n",
    "$\\newcommand{\\cL}{\\mathcal{L}}$\n",
    "$\\newcommand{\\cX}{\\mathcal{X}}$\n",
    "$\\newcommand{\\wdh}{\\widehat}$\n",
    "$\\newcommand{\\wdt}{\\widetilde}$\n",
    "$\\newcommand{\\vdelta}{\\partial}$\n",
    "$\\newcommand{\\pd}[2]{\\frac{\\vdelta #1}{\\vdelta #2}}$\n",
    "$\\newcommand{\\lsi}[1]{\\left[#1\\right)}$\n",
    "$\\newcommand{\\rsi}[1]{\\left(#1\\right]}$\n",
    "$\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\range}[2]{\\set{#1, \\ldots, #2}}$\n",
    "$\\newcommand{\\such}{\\ \\big|\\ }$\n",
    "$\\let\\bs\\backslash$\n",
    "$\\let\\ra\\rightarrow$\n",
    "$\\let\\Ra\\Rightarrow$\n",
    "$\\let\\Lora\\Longrightarrow$\n",
    "$\\let\\Lra\\Leftrightarrow$\n",
    "$\\let\\la\\leftarrow$\n",
    "$\\let\\La\\Leftarrow$\n",
    "$\\let\\Lola\\Longleftarrow$\n",
    "$\\let\\Lolra\\Longleftrightarrow$\n",
    "$\\let\\ole\\overline$\n",
    "$\\newcommand{\\floor}[1]{\\left\\lfloor#1\\right\\rfloor}$\n",
    "$\\newcommand{\\ceil}[1]{\\left\\lceil#1\\right\\rceil}$\n",
    "$\\newcommand{\\ps}[1]{\\left(#1\\right)}$\n",
    "$\\newcommand{\\md}[1]{\\left|#1\\right|}$\n",
    "$\\newcommand{\\nm}[1]{\\left\\|#1\\right\\|}$\n",
    "$\\newcommand{\\tbr}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "$\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\System}[1]{\\left\\{\\begin{aligned}#1\\end{aligned}\\right.}$\n",
    "$\\DeclareMathOperator{\\tr}{tr}$\n",
    "$\\DeclareMathOperator{\\cl}{cl}$\n",
    "$\\DeclareMathOperator{\\rk}{rk}$\n",
    "$\\DeclareMathOperator{\\Int}{int}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\Err}{Err}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91a6e3",
   "metadata": {},
   "source": [
    "Далее слово 'автор' относится исключительно к автору какой-либо статьи, а человек, который написал данный ноутбук, именутся как? И зачем?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b55347",
   "metadata": {},
   "source": [
    "## Причины, цели статьи\n",
    "\n",
    "### Основные цели\n",
    "- Машинное обучение (в частности GANы) нуждается в методах для решения вариациационных неравенств, которые бы требовали как можно меньше вызовов оракула (вычисление градиента, поиск проекции). Базовые методы требуют 2 вызова и градиента, и проекции за 1 итерацию. Авторы рассматривают сходимость модификаций экстра-градинентного метода, которые уменьшают число обращений к оракулу.\n",
    "- Получить те же результаты сходимости для предложенного метода, что и у экстра-градиентного спуска\n",
    "\n",
    "## Достижения\n",
    "- 1-EG алгоритмы сохраняют порядок сходимости $O(1 / t)$\n",
    "- Показано, что в случае немонотонного вариационного неравенства с высокой вероятностью последняя итерация 1-EG алгоритма будет удовлетворять локальной сходимости тоже порядка $O(1 / t)$ (потенциально первый в мире подобный результат!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83fc67",
   "metadata": {},
   "source": [
    "## Основные определения\n",
    "\n",
    "### Стохастический анализ\n",
    "\n",
    "Пусть $(\\Omega, \\cF, P)$ - вероятностной пространство, $(E, \\cE)$ - измеримое пространство. Тогда множество случайных величин $\\{X_t\\}_{t = 1}^\\infty$, $X_t \\colon \\Omega \\to E$ называется *стохастическим (случайным) процессом*.\n",
    "\n",
    "В силу исторических причин, $t$ соотносится обычно со временем.\n",
    "\n",
    "### Оптимизация\n",
    "\n",
    "Далее везде мы живём в пространстве $\\R^d$, если не сказано явно иного. Также мы закрепляем следующие обозначения:\n",
    "\n",
    "- $\\cX \\subseteq \\R^d$ за непустым замкнутым выпуклым подмножеством точек\n",
    "- $V \\colon \\R^d \\to \\R^d$ - некоторый оператор на нашем пространстве, чьи свойства мы будем уточнять по мере необходимости.\n",
    "\n",
    "**Определение:** *седловой точкой* дифференцируемой функции $f \\colon \\R^d \\to \\R^n$ называется критическая точка $f$, которая не является ни локальным минимумом, ни локальным максимумом $f$.\n",
    "\n",
    "**Определение:** Пусть $\\cX = \\Theta \\times \\Phi$, где $\\Theta \\subseteq \\R^{d_1}$ и $\\Phi \\subseteq \\R^{d_2}$. $\\cL(\\theta, \\phi)$ - дифференцируемая функция двух переменных с липшицевым градиентом. Тогда *min-max задачей* называется следующая задача оптимизации:\n",
    "$$\n",
    "    \\min_{\\theta \\in \\R^{d_1}} \\max_{\\phi \\in \\R^{d_2}} \\cL(\\theta, \\phi), \\text{ s.t. } \\theta \\in \\Theta \\wedge \\phi \\in \\Phi\n",
    "$$\n",
    "\n",
    "**Определение:** оператор $V$ называется *монотонным*, если выполнено следующее условие:\n",
    "$$\n",
    "    \\forall x, x' \\in \\R^d\\ \\ \\tbr{V(x') - V(x), x' - x} \\ge 0\n",
    "$$\n",
    "\n",
    "**Интуиция:** вектор изменения значений $V$ сонаправлен с осью, задаваемой вектором $x' - x$. Для случая $V = \\nabla f$, где $f$ некоторая *достаточно гладкая* функция, выполнение условия эквивалентно тому, что $f$ выпукла\n",
    "\n",
    "**Определение:** оператор $V$ называется *$\\alpha$-строго монотонным*, если выполнено следующее условие:\n",
    "$$\n",
    "    \\exists \\alpha > 0 \\such \\forall x, x' \\in \\R^d\\ \\ \\tbr{V(x') - V(x), x' - x} \\ge \\alpha\\|x' - x\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d7daf",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "**Определение:** *вариационным неравенством (variational inequality, или просто VI)* называется задача поиска такого $x^* \\in \\cX$, что выполнено неравенство:\n",
    "$$\n",
    "    \\exists x^* \\in \\cX \\such \\forall x \\in \\cX\\ \\ \\tbr{V(x^*), x - x^*} \\ge 0\n",
    "$$\n",
    "\n",
    "**Интуиция к определению:** мы уже сталкивались с похожим неравенством, когда говорили о задаче минимизации выпуклой дифференцируемой функции $f$ на некотором выпуклом множестве $\\cX$. Тогда получалось, что $x^*$ - искомый минимум тогда и только тогда, когда выполнено неравенство:\n",
    "$$\n",
    "    \\forall x \\in \\cX \\tbr{\\nabla f(x^*), x - x^*} \\ge 0\n",
    "$$\n",
    "По сути это означает, что множество $\\cX$ лежит по одну сторону от гиперплоскости, которая задаётся своей точкой $x^*$ и нормалью $\\nabla f(x^*)$\n",
    "\n",
    "<img src=\"assets/FullConvexTheorem.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "**Примеры:**\n",
    "\n",
    "1. Задача минимизации. Если $f$ - гладкая функция потерь на $\\cX = \\R^d$, то при $V = \\nabla f$ точка $x^*$ является решением вариационного неравенства тогда и только тогда, когда $\\nabla f(x^*) = 0$.\n",
    "\n",
    "2. Min-max оптимизация. Пусть $\\cX = \\Theta \\times \\Phi$, $\\Theta = \\R^{d_1}$ и $\\Phi = \\R^{d_2}$ и задана гладкая функция $L(\\theta, \\phi)$. Тогда задачу min-max оптимизации можно переписать в виде вариационного неравенства, используя $V(x) = (\\nabla_\\theta \\cL(x), -\\nabla_\\phi \\cL(x))$\n",
    "\n",
    "**Определение:** *функцией ошибки* для потенциального решения $\\wdh{x} \\in \\cX$ вариационного неравенства мы назовём следующую функцию:\n",
    "$$\n",
    "    \\Err(\\wdh{x}) = \\sup_{x \\in \\cX} \\tbr{V(x), \\wdh{x} - x}\n",
    "$$\n",
    "и её *ограниченный* вариант:\n",
    "$$\n",
    "    \\Err_R(\\wdh{x}) = \\max_{x \\in \\cX_R} \\tbr{V(x), \\wdh{x} - x}\n",
    "$$\n",
    "где $\\cX_R = \\cX \\cap \\ole{B}_R(0) = \\{x \\in \\cX \\colon \\|x\\| \\le R\\}$\n",
    "\n",
    "**Интуиция:** в определении задачи вариационного неравенства можно выделить часть, которая зависит не только от $x^*$. Вполне логично положить её за ошибку, ведь чем ближе мы оказываемся к решению, тем ближе должен быть максимум этого значения к нулю.\n",
    "\n",
    "**Определение:** будем говорить, что решение вариационного неравенства $x^* \\in \\cX$ является *регулярным*, если выполнены условия:\n",
    "1. Оператор $V$ является $C^1$-гладким в окрестности $x^*$\n",
    "2. Якобиан $J_V(x^*)$ является положительно определённым относительно всех касательных направлений к $\\cX$ в точке $x^*$ (множество таких направлений обозначил за $Z_{\\cX, x^*}$):\n",
    "$$\n",
    "    \\forall z \\in Z_{\\cX, x^*} \\bs \\{0\\}\\ \\ z^TJ_V(x^*)z = \\sum_{i = 1}^d \\sum_{j = 1}^d z_i \\pd{V_i}{x_j}(x^*)z_j > 0\n",
    "$$\n",
    "\n",
    "**Интуиция:** эти условия соотносятся с требованиями на гессиан, если мы рассматриваем задачу $V = \\nabla f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5371e16",
   "metadata": {},
   "source": [
    "**Лемма (Нестеров, 2007):** пусть $V$ монотонна. Тогда $x^* \\in \\cX$ является решением вариационного неравенства тогда и только тогда, когда $\\Err(x^*) = 0$.\n",
    "\n",
    "Более того, так как $\\Err(x^*) = 0$ влечёт за собой $\\Err_R(x^*) = 0$ для достаточно большого $R > 0$, то наличие условия $\\Err_R(x^*) = 0$ также является достаточным для того, чтобы $x^* \\in \\cX_R$ было решением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40071f",
   "metadata": {},
   "source": [
    "## Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c5688",
   "metadata": {},
   "source": [
    "### Допущения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd6324",
   "metadata": {},
   "source": [
    "**Допущение 1:** множество решений вариационного неравенства $\\cX*$ не является пустым\n",
    "\n",
    "**Допущение 2:** Оператор $V$ монотонен\n",
    "\n",
    "**Допушение 3:** Оператор $V$ является $\\beta$-липшицевым, иначе говоря:\n",
    "$$\n",
    "    \\forall x, x' \\in \\R^d\\ \\ \\|V(x') - V(x)\\| \\le \\beta\\|x' - x\\|\n",
    "$$\n",
    " \n",
    "**Допущение 4:** в некоторых случаях мы также будем требовать, что оператор $V$ является $\\alpha$-строго монотонным, но обычно это будет обговорено явно\n",
    "\n",
    "**Допущение:** Далее мы столкнёмся с изучением *стохастических (случайных) алгоритмов*. Все они будут генерировать последовательности точек $X_t$ из пространства $\\cX$ (говоря явно, $X_t \\colon \\Omega \\to \\cX$, где $\\Omega$ относится к множеству исходов вероятностного пространства). Случайность возникает из-за использования *стохастического оракула*, чьё возвращаемое значение можно задать так:\n",
    "$$\n",
    "    V_t = V(X_t) + Z_t\n",
    "$$\n",
    "где $Z_t \\in \\R^d$ - случайный шум. Если $\\cF_t$ - это натуральная фильтрация $X_t$, то мы допускаем следующие условия:\n",
    "1. $\\forall t\\ \\ \\E(Z_t | \\cF_t) = 0$\n",
    "\n",
    "2. $\\forall t\\ \\ \\E(\\|Z_t\\|^2| \\cF_t) \\le \\sigma^2$\n",
    "\n",
    "Понятно, что если $\\sigma^2 = 0$, то случайный процесс вырождается в детерминированный, всегда верно равенство $V_t = V(X_t)$. Далее мы будем использовать переменные случайных точек, но алгоритмы, описанные таким образом, естественно можно рассматривать как детерминированные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ccd8f",
   "metadata": {},
   "source": [
    "### Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c95e7",
   "metadata": {},
   "source": [
    "**Экстра-градиентный спуск (Extra-Gradient algorithm):** (t + 1)-итерация для EG-алгоритма записывается следующим образом:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        &{X_{t + 1 / 2} = \\Pi_\\cX(X_t - \\gamma_tV_t)}\n",
    "        \\\\\n",
    "        &{X_{t + 1} = \\Pi_\\cX(X_t - \\gamma_tV_{t + 1 / 2})}\n",
    "    \\end{aligned}\n",
    "$$\n",
    "где \n",
    "- $\\Pi_\\cX$ - обыкновенная евклидова проекция на $\\cX$:\n",
    "  $$\n",
    "      \\Pi_\\cX(y) := \\arg \\min_{x \\in \\cX} \\|y - x\\|\n",
    "  $$\n",
    "- $\\gamma_t > 0$ - произвольно выбранный шаг итерации\n",
    "\n",
    "**Замечание:** отличие от обычного градиентного спуска состоит в том, что здесь мы находим *промежуточную точку (её ещё называют ведущей)* $X_{t + 1 / 2}$, а затем по информации в ней строим следующую *базовую точку* $X_t$, используя при этом один и тот же шаг $\\gamma_t$. В своей идее экстраградиентный метод крайне напоминает метод Нестерова.\n",
    "\n",
    "**Экстра-градиентный спуск с одним вызовом (оракула):** существуют разные способы убрать градиент из итерации. Так, в статье рассмотрены следующие 3 (они используют либо предыдущий, уже посчитанный градиент, либо аппроксимируют его через разность уже имеющихся $X_t$):\n",
    "1. Экстра-градиентный спуск по прошлому оракулу (Past Extra-Gradient, PEG):\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{t + 1 / 2} = \\Pi_\\cX(X_t - \\gamma_tV_{t - 1 / 2})}\n",
    "            \\\\\n",
    "            &{X_{t + 1} = \\Pi_\\cX(X_t - \\gamma_tV_{t + 1 / 2})}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    **Идея:** используем $V_{t - 1 / 2}$ вместо того, чтобы считать $V_t$ при вычислении $X_{t + 1 / 2}$\n",
    "2. Отражённый градиентный спуск (Reflected Gradient, RG):\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{t + 1 / 2} = X_t - (X_{t - 1} - X_t)}\n",
    "            \\\\\n",
    "            &{X_{t + 1} = \\Pi_\\cX(X_t - \\gamma_tV_{t + 1 / 2})}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    **Идея:** используем $\\frac{X_{t - 1} - X_t}{\\gamma_t}$ вместо $V_t$ и отказываемся от проекции при вычислении $X_{t + 1 / 2}$\n",
    "3. Оптимистичный градиентный спуск (Optimistic Gradient, OG):\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{t + 1 / 2} = \\Pi_\\cX(X_t - \\gamma_tV_{t - 1 / 2})}\n",
    "            \\\\\n",
    "            &{X_{t + 1} = X_{t + 1 / 2} + \\gamma_tV_{t - 1 / 2} - \\gamma_tV_{t + 1 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    **Идея:** используем $V_{t - 1 / 2}$ вместо $V_t$ при вычислении $X_{t + 1 / 2}$. Используем $X_{t + 1 / 2} + \\gamma_tV_{t - 1 / 2}$ вместо $X_t$ и отказываемся от проекции при вычислении $X_{t + 1}$\n",
    "    \n",
    "Далее, чтобы говорить об этих методах сразу, мы будем называть их *1-EG методами* или *1-EG алгоритмами*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba6029",
   "metadata": {},
   "source": [
    "**Утверждение:** если все 1-EG детерминированные методы имеют одинаковые стартовые значения $X_0 = X_1 \\in \\cX$, $V_{1 / 2} = 0$ и запускаются с постоянным шагом $\\gamma_t = \\gamma$, то в случае $\\cX = \\R^d$ точки $X_t$ на каждой итерации у всех 1-EG совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95c6ad",
   "metadata": {},
   "source": [
    "**Доказательство:** индукция по $t$. Отметим, что коль скоро $\\cX = \\R^d$, то оператор проекции является просто тождественным, его можно не писать:\n",
    "- База индукции $t \\le 2$:\n",
    "    - PEG:\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{3 / 2} = X_1 - \\gamma V_{1 / 2} = X_0}\n",
    "            \\\\\n",
    "            &{X_2 = X_1 - \\gamma V_{3 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    - RG:\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{3 / 2} = X_1 - (X_0 - X_1) = X_1 = X_0}\n",
    "            \\\\\n",
    "            &{X_2 = X_1 - \\gamma V_{3 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "    - OG:\n",
    "    $$\n",
    "        \\begin{aligned}\n",
    "            &{X_{3 / 2} = X_1 - \\gamma V_{1 / 2} = X_0}\n",
    "            \\\\\n",
    "            &{X_2 = X_{3 / 2} + \\gamma V_{1 / 2} - \\gamma V_{3 / 2} = X_1 - \\gamma V_{3 / 2}}\n",
    "        \\end{aligned}\n",
    "    $$\n",
    "- Переход $t > 2$: покажем, что правила приводятся к одинаковому виду, тем самым точки получатся одинаковые.\n",
    "    - PEG:\n",
    "      $$\n",
    "          \\begin{aligned}\n",
    "              &{X_{t + 1 / 2} = X_t - \\gamma V_{t - 1 / 2}}\n",
    "              \\\\\n",
    "              &{X_{t + 1} = X_t - \\gamma V_{t + 1 / 2}}\n",
    "          \\end{aligned}\n",
    "      $$\n",
    "    - RG:\n",
    "      $$\n",
    "          \\begin{aligned}\n",
    "              &{X_{t + 1 / 2} = X_t - (X_{t - 1} - X_t)}\n",
    "              \\\\\n",
    "              &{X_{t + 1} = X_t - \\gamma V_{t + 1 / 2}}\n",
    "          \\end{aligned}\n",
    "      $$\n",
    "      Преобразуем первое правило. Для этого распишем $X_t$ через второе правило:\n",
    "      $$\n",
    "          X_t = X_{t - 1} - \\gamma V_{t - 1 + 1 / 2} = X_{t - 1} - \\gamma V_{t - 1 / 2} \\Lora X_{t - 1} - X_t = \\gamma V_{t - 1 / 2}\n",
    "      $$\n",
    "      Подстановка тривиально делает первое правило RG таким же, как и у PEG'а\n",
    "    - OG:\n",
    "      $$\n",
    "          \\begin{aligned}\n",
    "              &{X_{t + 1 / 2} = X_t - \\gamma V_{t - 1 / 2} \\Lora X_{t + 1 / 2} + \\gamma V_{t - 1 / 2} = X_t}\n",
    "              \\\\\n",
    "              &{X_{t + 1} = X_{t + 1 / 2} + \\gamma V_{t - 1 / 2} - \\gamma V_{t + 1 / 2} \\Lora X_{t + 1} = X_t - \\gamma V_{t + 1 / 2}}\n",
    "          \\end{aligned}\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b13fd8",
   "metadata": {},
   "source": [
    "### Технические леммы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6829346",
   "metadata": {},
   "source": [
    "**Замечание:** дальше будет довольно удобно использовать нотацию для подстановки переменных. Так, выражение $(x_1, \\ldots, x_n) \\la (y_1, \\ldots, y_n)$ означает подстановку вместо переменных $x_1, \\ldots, x_n$ переменные $y_1, \\ldots, y_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecb89b",
   "metadata": {},
   "source": [
    "**Техлемма 1:** пусть $x, y \\in \\R^d$ и $\\cC \\subseteq \\R^d$ - замкнутое выпуклое множество. Определим $x^+ := \\Pi_\\cC(x - y)$. Тогда верно следующее неравенство:\n",
    "$$\n",
    "    \\forall p \\in \\cC\\ \\ \\|x^+ - p\\|^2 \\le \\|x - p\\|^2 - 2\\tbr{y, x^+ - p} - \\|x^+ - x\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0ec3b",
   "metadata": {},
   "source": [
    "**Доказательство:** распишем величину слева в неравенстве:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        &\\|x^+ - p\\|^2 = \\|x^+ - x + x - p\\|^2 =\n",
    "        \\\\\n",
    "        &\\|x - p\\|^2 + 2\\tbr{x^+ - x, x - p} + \\|x^+ - x\\|^2 =\n",
    "        \\\\\n",
    "        &\\|x - p\\|^2 + 2\\tbr{x^+ - x, x^+ - p} + \\|x^+ - x\\|^2\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Проекция, как известно, обладает хорошим свойством на скалярное произведение: $\\tbr{x^+ - (x - y), x^+ - p} \\le 0$. Из этого неравенства выражаем то скалярное произведение, что наисано выше и тривиально получаем требуемое."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5744074",
   "metadata": {},
   "source": [
    "**Техлемма 2:** пусть $x, y_1, y_2 \\in \\R^d$ и $\\cC_1, \\cC_2 \\subseteq \\R^d$ - замкнутые выпуклые множества. Определим $x_i^+ := \\Pi_{\\cC_i}(x - y_i)$. Имеет место два факта:\n",
    "\n",
    "1. Если $\\cC_2 = \\R^d$, то\n",
    "   $$\n",
    "       \\forall p \\in \\R^d\\ \\ \\|x_2^+ - p\\|^2 = \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2\n",
    "   $$\n",
    "   \n",
    "2. Если $\\cC_2 \\subseteq \\cC_1$, то\n",
    "   $$\n",
    "       \\begin{aligned}\n",
    "           \\forall p \\in \\cC_2\\ \\ \\|x_2^+ - p\\|^2 &\\le \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + 2\\tbr{y_2 - y_1, x_1^+ - x_2^+} - \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2\n",
    "           \\\\\n",
    "           &\\le \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + \\|y_2 - y_1\\|^2 - \\|x_1^+ - x\\|^2\n",
    "       \\end{aligned}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc1417",
   "metadata": {},
   "source": [
    "**Доказательство:**\n",
    "\n",
    "1. Воспользуемся той же схемой доказательства, что и в Техлемме 1: просто распишем левую часть:\n",
    "   $$\n",
    "       \\begin{aligned}\n",
    "           \\|x_2^+ - p\\|^2 &= \\|x_2^+ - x_1^+ + x_1^+ - x + x - p\\|^2\n",
    "           \\\\\n",
    "           &= \\|x_2^+ - x_1^+\\|^2 + \\|x_1^+ - x\\|^2 + \\|x - p\\|^2\n",
    "           \\\\\n",
    "           &+ 2\\tbr{x_2^+ - x_1^+, x_1^+ - p} + 2\\tbr{x^+ - x, x - p}\n",
    "           \\\\\n",
    "           &= \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2 + \\|x - p\\|^2\n",
    "           \\\\\n",
    "           &+ 2\\tbr{x_2^+ - x_1^+, x_1^+ - p} + 2\\tbr{x_1^+ - x, x_1^+ - p}\n",
    "           \\\\\n",
    "           &= \\|x - p\\|^2 - 2\\tbr{y_2, x_1^+ - p} + \\|x_2^+ - x_1^+\\|^2 - \\|x_1^+ - x\\|^2\n",
    "       \\end{aligned}\n",
    "   $$\n",
    "   Равенство в 4й строке достигнуто за счёт того, что мы воспользовались преобразованием: $\\tbr{x^+ - x, x - p} = \\tbr{x^+ - x, x - x^+ + x^+ - p} = \\tbr{x^+ - x, x^+ - p} - \\|x^+ - x\\|^2$\n",
    "   А последнее равенство достигнуто при помощи $x_2^+ - x = -y_2$, коль скоро $\\cC_2 = \\R^d$\n",
    "   \n",
    "2. За счёт вложенности множеств, имеем $x_2^+ \\in \\cC_2 \\subseteq \\cC_1$. Значит, мы можем воспользоваться результатом Техлеммы 1 с подстановкой $(x, y, x^+, p, \\cC) \\la (x, y_1, x_1^+, x_2^+ \\cC_1)$. Вместе с другой подстановкой $(x, y, x^+, p, \\cC) \\la (x, y_2, x_2^+, p, \\cC_2)$ вначале это даёт 2 неравенства:\n",
    "   $$\n",
    "       \\begin{aligned}\n",
    "           \\|x_1^+ - x_2^+\\|^2 &\\le \\|x - x_2^+\\|^2 - 2\\tbr{y_1, x_1^+ - x_2^+} - \\|x_1^+ - x\\|^2\n",
    "           \\\\\n",
    "           \\|x_2^+ - p\\|^2 &\\le \\|x - p\\|^2 - 2\\tbr{y_2, x_2^+ - p} - \\|x_2^+ - x\\|^2\n",
    "       \\end{aligned}\n",
    "   $$\n",
    "   Если их сложить и перенести $\\|x_1^+ - x_2^+\\|^2$ в правую часть, то мы получаем в первое из неравенств пункта (по модулю того, что скалярные произведения нужно перегруппировать). Чтобы из него получить второе, нужно воспользоваться простеньким неравенством из неотрицательности нормы $\\|y_2 - y_1 + x_1^+ - x_2^+\\|^2 \\ge 0$:\n",
    "   $$\n",
    "       2\\tbr{y_2 - y_1, x_1^+ - x_2^+} \\le \\|y_2 - y_1\\|^2 + \\|x_1^+ - x_2^+\\|^2\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4846c6",
   "metadata": {},
   "source": [
    "**Техлемма 3:** пусть $\\{a_t\\}_{t = 1}^\\infty \\subset \\R$ и $b, t_0 \\in \\N$ таковы, что выполнено рекурсивное неравенство:\n",
    "$$\n",
    "    \\exists q > 1, q' > 0 \\such \\forall t \\ge t_0\\; a_{t + 1} \\le \\ps{1 - \\frac{q}{t + b}}a_t + \\frac{q'}{(t + b)^2}\n",
    "$$\n",
    "Тогда рекурсивное неравенство можно развернуть в следующее:\n",
    "$$\n",
    "    a_t \\le \\frac{q'}{q - 1} \\cdot \\frac{1}{t} + o\\ps{\\frac{1}{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e9e58",
   "metadata": {},
   "source": [
    "**Доказательство:** для начала покажем, что мы можем к изучению такой $a'_t \\le a_t$, что выполнено то же неравенство из условия, но без крайнего слагаемого:\n",
    "$$\n",
    "    \\forall t \\ge t_0\\; a'_{t + 1} \\le \\ps{1 - \\frac{q}{t + b}}a'_t\n",
    "$$\n",
    "**Далее идёт слишком синтетическое утверждение**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc5dc9",
   "metadata": {},
   "source": [
    "**Техлемма 4:** пусть $x^*$ - регулярное решение вариационного неравенства. Тогда, существуют константы $r, \\alpha, \\beta > 0$ такие, что выполнены свойства:\n",
    "- $V$ является $\\beta$-липшицевым на $K = \\ole{B}_r(x^*)$\n",
    "- $\\forall x \\in U := \\cX \\cap K\\ \\ \\tbr{V(x), x - x^*} \\ge \\alpha\\|x - x^*\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce63b4",
   "metadata": {},
   "source": [
    "**Доказательство:** существование окрестности, где $V$ является $\\beta$-липшицевым тривиально в силу того, что $V$ является $C^1$-гладкой функцией (а значит на компакте мы нужную константу найдём). Остаётся лишь найти $\\alpha$. Пусть $TC_\\cX(x^*)$ обозначает конус касательных направлений к $\\cX$ в точке $x^*$. Рассмотрим следующую функцию $\\phi \\colon \\R^{d \\times d} \\to \\R$:\n",
    "$$\n",
    "    \\phi(G) = \\min_{z \\in TC_\\cX(x^*) \\wedge \\|z\\| = 1} z^TGz\n",
    "$$\n",
    "Эта функция является выпуклой вверх, коль скоро представляет собой минимум от множества функций, линейных по тому же аргументу:\n",
    "$$\n",
    "    \\phi(\\theta G_1 + (1 - \\theta) G_2) = \\min_{z \\in TC_\\cX(x^*) \\wedge \\|z\\| = 1} \\underbrace{z^T(\\theta G_1 + (1 - \\theta) G_2)z}_{\\theta z^TG_1z + (1 - \\theta) z^TG_2z} \\ge \\theta \\phi(G_1) + (1 - \\theta) \\phi(G_2)\n",
    "$$\n",
    "В свою очередь, из-за этого факта $\\phi$ является непрерывной на внутренности своей эффективной области определения. В силу регулярности решения, $\\phi(J_V(x^*)) > 0$, а композиция $\\phi \\circ J_V$ непрерывна. Стало быть, можно найти $r, \\alpha > 0$ через поиск оценки $\\phi(J_V(x)) \\ge \\alpha$ для всех $x \\in U$. Итак, распишем разность значений $V$ через интеграл по якобиану (интеграл матрицы нужно понимать поэлементно):\n",
    "$$\n",
    "    V(x) - V(x^*) = \\ps{\\int_0^1 J_V\\big(x^* + \\lambda(x - x^*)\\big)d\\lambda}(x - x^*)\n",
    "$$\n",
    "Для упрощения вводим обозначения $z = x - x^* \\in TC_\\cX(x^*)$, $x_\\lambda = x^* + \\lambda(x - x^*) \\in K$. Тогда, взяв скалярное произведение с $z$ с обеих сторон, имеем:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\tbr{V(x) - V(x^*), z} &= \\tbr{V(x) - V(x^*), x - x^*} = z^T\\ps{\\int_0^1 J_V(x_\\lambda)d\\lambda}z\n",
    "        \\\\\n",
    "        &\\ge \\ps{\\int_0^1 \\phi(J_V(x_\\lambda))d\\lambda}\\|z\\|^2 \\ge \\alpha\\|z\\|^2 = \\alpha\\|x - x^*\\|^2\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Наконец, пользуемся тем, что $x^*$ является решением, а значит $\\tbr{V(x^*), x - x^*} \\ge 0$ и верна цепочка неравенств:\n",
    "$$\n",
    "    \\tbr{V(x), x - x^*} \\ge \\tbr{V(x) - V(x^*), x - x^*} \\ge \\alpha\\|x - x^*\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bd5f4",
   "metadata": {},
   "source": [
    "### Анализ детерминированного случая"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfbac2",
   "metadata": {},
   "source": [
    "Как и было обговорено ранее, мы считаем детерминированный случай просто вырожденным случаем для стохастического, где $\\sigma = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f97028",
   "metadata": {},
   "source": [
    "#### Эргодическое среднее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13e8aa",
   "metadata": {},
   "source": [
    "**Теорема о глобальной сходимости (для эргодического среднего):** пусть $V$ удовлетворяет допущениям 1-3. Если рассмотреть 1-EG метод с постоянным шагом $\\gamma < 1 / (c\\beta)$ (где $c = 1 + \\sqrt{2}$ для RG и $c = 2$ для PEG, OG). Тогда имеет место неравенство:\n",
    "$$\n",
    "    \\forall R > 0\\ \\ \\Err_R\\big(\\ole{X}_t\\big) \\le \\frac{1}{t} \\cdot \\frac{R^2 + \\|X_1 - X_{1 / 2}\\|^2}{2\\gamma}\n",
    "$$\n",
    "где в определении $\\Err_R$ берётся $\\cX_R = \\cX \\cap \\ole{B}_R(X_1)$, а также $\\ole{X}_t = \\frac{1}{t}\\sum_{s = 1}^t X_{s + 1 / 2}$ - эргодическое среднее по направляющим точкам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01d9f0",
   "metadata": {},
   "source": [
    "**Лемма:** если $V$ является монотонным оператором и удовлетворяет следующему неравенству:\n",
    "$$\n",
    "    \\exists \\mu_s, \\lambda_s \\ge 0 \\such \\forall p \\in \\cX_R, s \\in \\range{1}{t}\\; \\|X_{s + 1} - p\\|^2 \\le \\|X_s - p\\|^2 - 2\\lambda_s\\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} + \\mu_s - \\mu_{s + 1}\n",
    "$$\n",
    "где $\\cX_R = \\cX \\cap \\ole{B}_R(X_1)$. Тогда имеет место следующее неравенство для ошибки:\n",
    "$$\n",
    "    \\Err_R \\ps{\\ole{X}} \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$\n",
    "где $\\ole{X} = \\frac{\\sum_{s = 1}^t \\lambda_sX_{s + 1 / 2}}{\\sum_{s = 1}^t \\lambda_s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7a298",
   "metadata": {},
   "source": [
    "**Доказательство леммы:** как и во многих других случаях с эргодическим средним, мы будем делать телескопическую сумму, в чём нам особенно хорошо помогает тот объект, который дан в условии. Перенесём скалярное произведение в левую сторону и просуммируем:\n",
    "$$\n",
    "    \\sum_{s = 1}^t 2\\lambda_s\\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\le \\|X_1 - p\\|^2 - \\|X_{t + 1} - p\\|^2 + \\mu_1 - \\mu_{t + 1} \\le \\|X_1 - p\\|^2 + \\mu_1\n",
    "$$\n",
    "Согласно условию, $p \\in \\cX_R = \\cX \\cap \\ole{B}_R(X_1)$, а значит автоматически $\\|X_1 - p\\|^2 \\le R^2$. Более того, по монотонности $V$ мы можем преобразовать сумму слева:\n",
    "$$\n",
    "    \\tbr{V(X_{s + 1 / 2}) - V(p), X_{s + 1 / 2} - p} \\ge 0 \\Lora \\tbr{V(p), X_{s + 1 / 2} - p} \\le \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p}\n",
    "$$\n",
    "Теперь соберём всё вместе:\n",
    "$$\n",
    "    \\sum_{s = 1}^t 2\\lambda_s\\tbr{V(p), X_{s + 1 / 2} - p} \\le R^2 + \\mu_1\n",
    "$$\n",
    "Поделим обе стороны на 2, занесём сумму под скалярное произведение, а также поделим на $\\sum_{s = 1}^t \\lambda_s$:\n",
    "$$\n",
    "    \\tbr{V(p), \\frac{\\sum_{s = 1}^t \\lambda_sX_{s + 1 / 2}}{\\sum_{s = 1}^t \\lambda_s} - p} \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$\n",
    "Максимизация по $p$ приводит значение левой части к значению $\\Err_R$ с соответствующим аргументом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7d0c4",
   "metadata": {},
   "source": [
    "**Доказательство теоремы:** вся идея состоит в том, чтобы свести по отдельности PEG, OG и RG к доказанной лемме. Нужно отметить, что мы будем требовать для PEG и OG инициализацию с любым $X_{1 / 2}$ и $X_1 \\in \\cX$, в то время как для RG достаточно просто точек $X_0$ и $X_{1 / 2}$. К сожалению, каждый метод нужно сводить отдельно:\n",
    "\n",
    "- **PEG.** \n",
    "  1. Подстановка параметров во 2е неравенство Техлеммы 2, использование липшцевости $V$\n",
    "  2. За счёт неравенства Юнга и свойства нерасширяемости проекции получить неравенство на норму одного слагаемого из первого пункта.\n",
    "  3. Подготовить неравенство предыдущего пункта для телескопии при подстановке в первый пункт\n",
    "  4. Отдельно рассмотреть случай $t = 1$\n",
    "\n",
    "- **OG.**\n",
    "  1. Подстановка параметров в 1е неравенство Техлеммы 2\n",
    "  2. Подставив неравенства из свойства разделения проекции, липшицевости $V$ и того же неравенства Юнга в первый пункт, получить искомое неравенство\n",
    "\n",
    "- **RG.**\n",
    "  1. Подстановка параметров в 2е неравенство Техлеммы 2\n",
    "  2. Увидеть 2 неравенства из свойства разделения проекции, просуммировать их.\n",
    "  3. Сделать оценку на единственное скалярное произведение из пункта 1, собрать это воедино\n",
    "  4. Дважды неравенство Юнга для произведения модулей из третьего пункта. Собрать воедино"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545b646",
   "metadata": {},
   "source": [
    "**Улучшение леммы в задачах минизации и min-max**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c09592",
   "metadata": {},
   "source": [
    "Лемма 2 может быть улучшена для конкретных задач: получаются другие функции ошибок, которые могут лучше отражать специфику задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d1553",
   "metadata": {},
   "source": [
    "**Задача минимизации:** $V = \\nabla f$. В силу монотонности $V$ по условию, сама $f$ является выпуклой. Отсюда для любой точки $p \\in \\cX$ по неравенству Йенсена получаем:\n",
    "$$\n",
    "    \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} = \\tbr{\\nabla f(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\ge f(X_{s + 1 / 2}) - f(p)\n",
    "$$\n",
    "Объединив это с рассматриваемым средневзвешенным в лемме, получим следующее:\n",
    "$$\n",
    "    \\frac{1}{\\sum_{s = 1}^t \\lambda_s} \\sum_{s = 1}^t \\lambda_s \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\ge \\frac{1}{\\sum_{s = 1}^t \\lambda_s} \\sum_{s = 1}^t \\lambda_s f(X_{s + 1 / 2}) - f(p) \\ge f(\\ole{X}) - f(p)\n",
    "$$\n",
    "Применим это неравенство к $p \\in \\cX^*$. Тогда, используя первое неравенство из доказательства леммы с $R = \\rho(X_1, \\cX^*)$, мы получаем следующий результат:\n",
    "$$\n",
    "    f(\\ole{X}) - f^* \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a13411",
   "metadata": {},
   "source": [
    "**Задача min-max:** $V = (\\nabla_\\theta \\cL, -\\nabla_\\phi \\cL)$. Монотонность такой $V$ эквивалентна тому, что $\\cL$ является выпуклой по $\\theta$ и вогнутой по $\\phi$. Обозначим $X_{s + 1 / 2} = (\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2})$ и $p = (\\theta, \\phi)$. За счёт уже упомянутой выпуклости-вогнутости $\\cL$, мы получаем следующее неравенство:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} &= \\tbr{\\nabla_\\theta \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2}), \\theta_{s + 1 / 2} - \\theta} - \\tbr{\\nabla_\\phi \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2}), \\phi_{s + 1 / 2} - \\phi}\n",
    "        \\\\\n",
    "        &\\ge \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2}) - \\cL(\\theta, \\phi_{s + 1 / 2}) + \\cL(\\theta_{s + 1 / 2}, \\phi) - \\cL(\\theta_{s + 1 / 2}, \\phi_{s + 1 / 2})\n",
    "        \\\\\n",
    "        &= \\cL(\\theta_{s + 1 / 2}, \\phi) - \\cL(\\theta, \\phi_{s + 1 / 2})\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Как и ранее, теперь мы объединяем это со средневзвешенным (тут $\\ole{X} = (\\ole{\\theta}, \\ole{\\phi})$):\n",
    "$$\n",
    "    \\frac{1}{\\sum_{s = 1}^t \\lambda_s} \\sum_{s = 1}^t \\lambda_s \\tbr{V(X_{s + 1 / 2}), X_{s + 1 / 2} - p} \\ge \\cL(\\ole{\\theta}, \\phi) - \\cL(\\theta, \\ole{\\phi})\n",
    "$$\n",
    "Снова подставляем полученный результат в первое неравенство леммы. Максимизация по $p = (\\theta, \\phi) \\in \\cX \\cap \\ole{B}_R(X_1)$ даёт следующую оценку:\n",
    "$$\n",
    "    NI_R(\\ole{X}) \\le \\frac{R^2 + \\mu_1}{2\\sum_{s = 1}^t \\lambda_s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaed74f",
   "metadata": {},
   "source": [
    "#### Последняя итерация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141b7ef",
   "metadata": {},
   "source": [
    "\n",
    "**Теорема о глобальной сходимости (для последней итерации):** пусть $V$ удовлетворяет допущениям 1, 2 и 4. Пусть $x^*$ - единственное решение вариационного неравенства. Тогда, если 1-EG метод запущен с постоянным достаточно малым шагом $\\gamma$, то полученная последовательность точек $X_t$ сходится к $x^*$ со следущей асимптотикой:\n",
    "$$\n",
    "    \\exists \\rho > 0 \\such \\|X_t - x^*\\| = O\\big(\\exp(-\\rho t)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767a969",
   "metadata": {},
   "source": [
    "### Анализ стохастического случая"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d557c",
   "metadata": {},
   "source": [
    "К сожалению, случайные процессы будут только в следующем семестре, а значит осознание доказательств на текущем уровне невозможно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb331032",
   "metadata": {},
   "source": [
    "**Теорема о глобальной сходимости:** пусть выполнены допущения 1, 2 и 4. Если рассмотреть PEG-алгоритм со стохастическим оракулом, чьи ответы удовлетворяют модели, а также шаг имеет вид $\\gamma_t = \\gamma / (t + b)$ для некоторых $\\gamma > 1 / \\alpha$ и $b \\ge 4\\beta\\gamma$, то имеет место неравенство:\n",
    "$$\n",
    "    \\E\\big(\\|X_t - x^*\\|^2\\big) \\le \\frac{6\\gamma^2\\sigma^2}{\\alpha\\gamma - 1} \\cdot \\frac{1}{t} + o\\ps{\\frac{1}{t}}\n",
    "$$\n",
    "При этом, для эргодического среднего $\\ole{X}_t = \\frac{1}{t}\\sum_{s = 1}^t X_s$ тоже верно похожее неравенство:\n",
    "$$\n",
    "    \\E\\big(\\|\\ole{X}_t - x^*\\|^2\\big) \\le \\frac{6\\gamma^2\\sigma^2}{\\alpha\\gamma - 1} \\cdot \\frac{\\log t}{t} + o\\ps{\\frac{\\log t}{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3b441",
   "metadata": {},
   "source": [
    "**Теорема о локальной сходимости:** пусть $x^*$ - регулярное решение вариационного неравенства, $\\delta > 0$ - *уровень терпимости*. Если рассмотреть PEG-алгоритм со стохастическим оракулом, чьи ответы удовлетворяют модели, а также шаг имеет вид $\\gamma_t = \\gamma / (t + b)$ для некоторых $\\gamma > 1 / \\alpha$ и достаточно большого $b$, то имеют место утверждения:\n",
    "1. Существуют окрестности $U, U_1 \\subseteq \\cX$ около точки $x^*$ такие, что если $X_{1 / 2} \\in U$ и $X_1 \\in U_1$, то событие $E_\\infty = \\{\\forall t \\in \\N\\ \\ X_{t + 1 / 2} \\in U\\}$  (все ведущие точки лежат в окрестности $U$) происходит с вероятностью не менее $1 - \\delta$.\n",
    "\n",
    "2. В предположении первого пункта, имеет место неравенство:\n",
    "$$\n",
    "    \\E\\big(\\|X_t - x^*\\|^2 | E_\\infty\\big) \\le \\frac{4\\gamma^2(M^2 + \\sigma^2)}{(\\alpha\\gamma - 1)(1 - \\delta)} \\cdot \\frac{1}{t} + o\\ps{\\frac{1}{t}}\n",
    "$$\n",
    "где $M = \\sup_{x \\in U} \\|V(x)\\| < \\infty$ и $\\alpha = \\inf_{x \\in U} \\frac{\\tbr{V(x), x - x^*}}{\\|x - x^*\\|^2} > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4725a3",
   "metadata": {},
   "source": [
    "**Замечание к теореме:** конечность $M$ и положительность $\\alpha$ являются следствием регулярности $x^*$, а их значения зависят только на величине окрестности $U$. Так, с ростом окрестности $U$, которая отвечает за область почти достоверной сходимости, мы получаем более плохую сходимость, ибо $M$ неубывает с этим ростом, а $\\alpha$ может только уменьшится (это плохо, ибо оно в знаменателе). Схожим образом $U_1$ зависит только от $U$, причём имеет смысл рассматривать такие $U_1$, что они составляют четверть от $U$ (в смысле меры ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117e50b",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450ecc6",
   "metadata": {},
   "source": [
    "### Цели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249c066",
   "metadata": {},
   "source": [
    "1. Показать работу экстра-градиентных методов для решения задачи вариационного неравенства\n",
    "2. Показать, что рассмотренные 1-EG методы показывают себя качественно и количественно лучше чем стандартный экстраградиентный метод\n",
    "3. Попробовать выбрать лучший из предложенных методов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e175b",
   "metadata": {},
   "source": [
    "## Затронутая при анализе литература\n",
    "\n",
    "1. https://arxiv.org/abs/1908.08465\n",
    "2. https://arxiv.org/abs/2006.08141"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daEnv",
   "language": "python",
   "name": "daenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
